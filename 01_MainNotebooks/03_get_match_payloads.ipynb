{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a5f52b33-4dd4-4609-9c6a-6ef248415876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sys\n",
    "import concurrent.futures \n",
    "from typing import Union, Dict, List, Tuple, Any\n",
    "import glob \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3a6c1058-028a-4382-9e9d-ffbdaa5f5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script fetches the raw JSON containing all \"match payloads\" containing match metadata\n",
    "    for a given WTT event ID using a GET request.\n",
    "    \n",
    "    Match-codes contained in payload are reequired for subsequent API call to get full match details.\n",
    "    \n",
    "    Reverse engineered from WTT events pages such as:\n",
    "    https://www.worldtabletennis.com/eventInfo?eventId=3085&selectedTab=Matches\n",
    "\n",
    "    Events_file is a csv containing the events list of events to be scraped based on their unique event ID.\n",
    "\n",
    "    A csv file is made for each event containing all match payloads for that event.\n",
    "\n",
    "    Threading has been implemented to speed up the proccess.\n",
    "    \n",
    "\"\"\"\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# Specifying the csv containing all the events from\n",
    "EVENTS_FILE = \"../Data/Processed/Events/shortlist_events.csv\"\n",
    "\n",
    "# A csv for each event containing its match payloads will be saved to this directory/\n",
    "OUTPUT_DIR = \"../Data/Raw/Match_payloads\"\n",
    "\n",
    "# Values used to generate random pause duration in seconds for API politeness\n",
    "MIN_PAUSE = 0.1 \n",
    "MAX_PAUSE = 0.2 \n",
    "\n",
    "# Number of threads for the IO processing.\n",
    "# Based on reading - 20 is a good starting number:\n",
    "MAX_WORKERS = 20 \n",
    "\n",
    "# Max retries for fdailed requests\n",
    "MAX_RETRIES = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3776bfc0-dfdb-47e6-aa40-da7173780d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_save_payloads(event_id: Union[int, str], output_dir: str, min_pause: float, max_pause: float) -> Tuple[int, bool, int, str]:\n",
    "    \"\"\"\n",
    "    For one event: fetches match payloads, saves to CSV, and handles errors and reporting.\n",
    "    For use with threadpool workers\n",
    "    Returns: (event_id, status_bool, match_count, status_message)\n",
    "    \"\"\"\n",
    "\n",
    "    # Define API endpoint URL and necessary params + headers.\n",
    "    url = \"https://liveeventsapi.worldtabletennis.com/api/cms/GetOfficialResult\"\n",
    "    params = {'EventId': str(event_id), \"DocumentCode\": \"TTE\"}\n",
    "    headers = {\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Referer': 'https://www.worldtabletennis.com/',\n",
    "        'User-Agent': 'Mozilla/5.0 (Linux; Android 11.0; Surface Duo) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Mobile Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # initialise variables for match_count and the status message for logging.\n",
    "    match_count = 0\n",
    "    status_msg = \"\"\n",
    "\n",
    "    # Try the api call and get response as json. Timeout set to keep thread running if API TimeoutError occurs\n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers, timeout=20)\n",
    "        \n",
    "        # Raise exception for bad status codes (4xx request errors or 5xx server errors)\n",
    "        response.raise_for_status()\n",
    "        raw_payloads = response.json() \n",
    "\n",
    "        # Check if response is a list as expected.\n",
    "        if not isinstance(raw_payloads, list):\n",
    "            # return content for logging, type.__name__ isolates typename as string  \n",
    "            status_msg = f\"JSON was not a list ({type(raw_payloads).__name__})\"\n",
    "            # return content for logginga\n",
    "            return event_id, False, 0, status_msg\n",
    "\n",
    "        # convert match payloads to df and get length / number of matches\n",
    "        payloads_df = pd.DataFrame(raw_payloads)\n",
    "        match_count = len(payloads_df)\n",
    "\n",
    "        # Create filename and save df to csv (even if content is blank)\n",
    "        filename = os.path.join(OUTPUT_DIR,f\"{event_id}_match_payloads.csv\")\n",
    "        payloads_df.to_csv(filename, index=False)\n",
    "\n",
    "        sleep_duration = random.uniform(min_pause, max_pause)\n",
    "        time.sleep(sleep_duration)\n",
    "\n",
    "        return event_id, True, match_count, f\"Found {match_count} matches for event:{event_id}. Pausing for {sleep_duration:.1f}s.\"\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        status_msg = f\"HTTP Error: {e.response.status_code}\"\n",
    "    except Exception as e:\n",
    "        status_msg = f\"Error: {type(e).__name__}\"\n",
    "\n",
    "    return event_id, False, 0, status_msg \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c2f3ec43-49ff-425b-a5ea-0f9489c64c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_events_to_scrape(shortlist_df: pd.DataFrame, output_dir: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Checks the event_shortlist and output directory to return event_ids\n",
    "    that need to be scraped. An event needs scraping if it is not completed OR\n",
    "    if its payload file does not exist.\n",
    "\n",
    "    Args:\n",
    "        shortlist_df (pd.DataFrame): Shortlist of events to be scraped (must contain 'eventId' and 'EventStatus').\n",
    "        output_dir (str): The directory where existing match payloads are saved.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A series of event_ids that need to be scraped.\n",
    "    \"\"\"\n",
    "\n",
    "    events_total_initial = len(shortlist_df)\n",
    "\n",
    "    # --- Pre-flight Check ---\n",
    "    required_cols = ['eventId', 'EventStatus']\n",
    "    if not all(col in shortlist_df.columns for col in required_cols):\n",
    "        print(f\"--- âŒ ERROR: Shortlist DataFrame is missing one of {required_cols}. Cannot proceed. ---\")\n",
    "        return pd.Series([], dtype=int) # Return empty Series\n",
    "\n",
    "    ids_to_scrape: List[int] = []\n",
    "    events_checked_count = 0\n",
    "\n",
    "    print(f\"\\n--- ğŸŸ  Starting Check on {events_total_initial} events to determine scrape list... ğŸŸ  ---\")\n",
    "\n",
    "    # loop through the shortlist dataframe\n",
    "    for index, event_row in shortlist_df.iterrows():\n",
    "        events_checked_count += 1\n",
    "        event_id = event_row['eventId']\n",
    "        \n",
    "        # --- FIX 1: Get the actual string status ---\n",
    "        event_status_str = event_row['EventStatus'] \n",
    "\n",
    "        # 1. Skip 'Future' events (they have no data to scrape yet)\n",
    "        if event_status_str == \"Future\":\n",
    "            continue\n",
    "\n",
    "        # 2. Check if the event is 'Ongoing'\n",
    "        is_ongoing = (event_status_str == \"Ongoing\")\n",
    "\n",
    "        # 3. Check if the payload file is missing\n",
    "        payload_file = os.path.join(output_dir, f\"{event_id}_match_payloads.csv\")\n",
    "        file_is_missing = not os.path.exists(payload_file)\n",
    "\n",
    "        # 4. Scrape if (Ongoing) OR (File is Missing)\n",
    "        #    This is the core logic you defined.\n",
    "        if is_ongoing or file_is_missing:\n",
    "            ids_to_scrape.append(event_id)  \n",
    "   \n",
    "    \n",
    "    events_to_scrape_count = len(ids_to_scrape)\n",
    "    # Calculate skipped (completed or future)\n",
    "    already_obtained_and_completed_count = events_total_initial - events_to_scrape_count \n",
    "\n",
    "    print(f\"\\n--- CHECK COMPLETE: {events_to_scrape_count}/{events_total_initial} events identified for scraping. ---\")\n",
    "    print(f\"âœ… Total Events: {events_total_initial} | Skipped: {already_obtained_and_completed_count} | To Scrape: {events_to_scrape_count}\")\n",
    "\n",
    "    # Return the clean Series of IDs that need scraping\n",
    "    return ids_to_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5db6801e-6bb6-4163-902e-e3f50d3ec80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ğŸš€ Starting Obtaining Match Payloads ğŸš€---\n",
      "\n",
      "--- ğŸŸ  Starting Check on 371 events to determine scrape list... ğŸŸ  ---\n",
      "\n",
      "--- CHECK COMPLETE: 1/371 events identified for scraping. ---\n",
      "âœ… Total Events: 371 | Skipped: 370 | To Scrape: 1\n",
      "\n",
      "---ğŸš€ Starting Initial Concurrent Scraping for 1 Events ğŸš€---\n",
      "--- Using 20 threads. API pause: 0.1s - 0.2s ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 3191: (1/1)  âŒ Failed: Error: ReadTimeout\n",
      "\n",
      "--- Initial scraping phase complete. 0/1 succeeded initially. ---\n",
      "\n",
      "--- Starting retry number 1/10 for 1 failed events ---\n",
      "\n",
      "--- Retry Attempt 1 complete. 1/1 succeeded this attempt. ---\n",
      "\n",
      "==================================================\n",
      "âœ… Finished! Match payloads obtained for 1/1 requested events (including retries).\n",
      "0  matches found for new / ongoing events\n",
      "Total run time = 0 m and 23 s.\n",
      "---ğŸŸ¢ Scraping finished. ğŸŸ¢---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    start_time = time.time() # Start timing the entire run\n",
    "    print(\"---ğŸš€ Starting Obtaining Match Payloads ğŸš€---\")\n",
    "\n",
    "    #  Perform Skip Check on Shortlist DF and\n",
    "    try:\n",
    "        shortlist_df = pd.read_csv(EVENTS_FILE)              \n",
    "    except FileNotFoundError:\n",
    "        print(f\"--- âŒ ERROR: Shortlist file not found at {EVENTS_FILE}. ---\"); sys.exit(1)\n",
    "  \n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    events_to_scrape_ids = filter_events_to_scrape(shortlist_df, OUTPUT_DIR)\n",
    "    event_ids_to_process = events_to_scrape_ids\n",
    "    events_to_scrape_count = len(event_ids_to_process)\n",
    "\n",
    "    if events_to_scrape_count == 0:\n",
    "        print(\"\\n--- âœ… PROCESS COMPLETE: No events remaining to scrape. ---\"); sys.exit(0)\n",
    "\n",
    "    # --- Counters & Setup ---\n",
    "    processed_count = 0\n",
    "    successful_count = 0\n",
    "    failed_event_ids: List[int] = [] # Initial list for failures\n",
    "    start_time_loop = time.time()\n",
    "    new_payoads_count = 0\n",
    "\n",
    "    print(f\"\\n---ğŸš€ Starting Initial Concurrent Scraping for {events_to_scrape_count} Events ğŸš€---\")\n",
    "    \n",
    "    print(f\"--- Using {MAX_WORKERS} threads. API pause: {MIN_PAUSE:.1f}s - {MAX_PAUSE:.1f}s ---\")\n",
    "\n",
    "    # --- Initial Parallel Execution ---\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {\n",
    "            # Ensure MIN_PAUSE_S and MAX_PAUSE_S constants are passed\n",
    "            executor.submit(fetch_and_save_payloads, event_id, OUTPUT_DIR, MIN_PAUSE, MAX_PAUSE): event_id\n",
    "            for event_id in event_ids_to_process\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            processed_count += 1\n",
    "            event_id = futures[future]\n",
    "            try:\n",
    "                result_id, status, match_count, status_msg = future.result()\n",
    "                if status:\n",
    "                    successful_count += 1\n",
    "                    new_payoads_count += match_count\n",
    "                    # --- SUCCESS: No print ---\n",
    "                else:\n",
    "                    failed_event_ids.append(event_id)\n",
    "                    # --- FAILURE: Print error message ---\n",
    "                    print(f\"Event {event_id}: ({processed_count}/{events_to_scrape_count})  âŒ Failed: {status_msg}\")\n",
    "\n",
    "                # --- Checkpoint Log (Every 10 events) ---\n",
    "                if processed_count % 10 == 0:\n",
    "                    elapsed_time = time.time() - start_time_loop\n",
    "                    minutes = int(elapsed_time // 60); seconds = int(elapsed_time % 60)\n",
    "                    # Print checkpoint summary on a new line\n",
    "                    print(f\"\\n---  {processed_count}/{events_to_scrape_count} processed. Success Rate: {successful_count/processed_count:.1%}. Elapsed: {minutes}m {seconds}s ---\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Print fatal errors during result retrieval\n",
    "                print(f\"\\n--- âŒ FATAL ERROR processing result for Event {event_id}: {type(e).__name__} ---\")\n",
    "                failed_event_ids.append(event_id)\n",
    "\n",
    "    print(f\"\\n--- Initial scraping phase complete. {successful_count}/{events_to_scrape_count} succeeded initially. ---\")\n",
    "\n",
    "\n",
    "    # --- RETRY LOGIC BLOCK ---\n",
    "    retry_attempt = 0\n",
    "    while failed_event_ids and retry_attempt < MAX_RETRIES:\n",
    "        retry_attempt += 1\n",
    "        ids_to_retry = list(failed_event_ids)\n",
    "        failed_event_ids.clear()\n",
    "        current_retry_successful_count = 0\n",
    "        current_retry_processed_count = 0\n",
    "\n",
    "        # --- Print Retry Start ---\n",
    "        print(f\"\\n--- Starting retry number {retry_attempt}/{MAX_RETRIES} for {len(ids_to_retry)} failed events ---\")\n",
    "        time.sleep(2) # Small pause before retry batch\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor_retry:\n",
    "            futures_retry = {\n",
    "                \n",
    "                executor_retry.submit(fetch_and_save_payloads, event_id, OUTPUT_DIR, MIN_PAUSE, MAX_PAUSE): event_id\n",
    "                for event_id in ids_to_retry\n",
    "            }\n",
    "\n",
    "            for future_retry in concurrent.futures.as_completed(futures_retry):\n",
    "                current_retry_processed_count += 1\n",
    "                event_id_retry = futures_retry[future_retry]\n",
    "                try:\n",
    "                    result_id_retry, status_retry, match_count_retry, status_msg_retry = future_retry.result()\n",
    "                    if status_retry:\n",
    "                        current_retry_successful_count += 1\n",
    "                        new_payoads_count += match_count\n",
    "                        # --- SUCCESS (Retry): No print ---\n",
    "                    else:\n",
    "                        failed_event_ids.append(event_id_retry)\n",
    "                        # --- FAILURE (Retry): Print error message ---\n",
    "                        print(f\"Retry {retry_attempt}: Event {event_id_retry} ({current_retry_processed_count}/{len(ids_to_retry)}) âŒ Failed again: {status_msg_retry}\")\n",
    "\n",
    "                    # --- Checkpoint Log (Every 10 events within retry batch) ---\n",
    "                    # Note: Using current_retry_processed_count here\n",
    "                    if current_retry_processed_count % 10 == 0:\n",
    "                        elapsed_time = time.time() - start_time_loop # Still measure from loop start\n",
    "                        minutes = int(elapsed_time // 60); seconds = int(elapsed_time % 60)\n",
    "                        # Calculate success rate for *this retry batch*\n",
    "                        batch_success_rate = current_retry_successful_count / current_retry_processed_count if current_retry_processed_count > 0 else 0\n",
    "                        print(f\"\\n--- RETRY CHECKPOINT: {current_retry_processed_count}/{len(ids_to_retry)} processed in attempt {retry_attempt}. Batch Success: {batch_success_rate:.1%}. Total Elapsed: {minutes}m {seconds}s ---\")\n",
    "\n",
    "                except Exception as e_retry:\n",
    "                     print(f\"\\n--- âŒ FATAL ERROR during retry for Event {event_id_retry}: {type(e_retry).__name__} ---\")\n",
    "                     failed_event_ids.append(event_id_retry)\n",
    "\n",
    "        successful_count += current_retry_successful_count # Add successful retries to total\n",
    "        print(f\"\\n--- Retry Attempt {retry_attempt} complete. {current_retry_successful_count}/{len(ids_to_retry)} succeeded this attempt. ---\")\n",
    "        if failed_event_ids:\n",
    "            print(f\"--- {len(failed_event_ids)} events still failing after {retry_attempt} retries. ---\")\n",
    "\n",
    "    # --- End of Retry Logic ---\n",
    "\n",
    "\n",
    "    # --- Final Summary ---\n",
    "    total_run_time = time.time() - start_time\n",
    "    total_minutes = int(total_run_time // 60)\n",
    "    total_seconds = int(total_run_time % 60)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"âœ… Finished! Match payloads obtained for {successful_count}/{events_to_scrape_count} requested events (including retries).\")\n",
    "    print(f\"{new_payoads_count}  matches found for new / ongoing events\")\n",
    "    if failed_event_ids:\n",
    "        print(f\"âš ï¸ Permanently Failed Event IDs ({len(failed_event_ids)}): {failed_event_ids}\")\n",
    "    print(f\"Total run time = {total_minutes} m and {total_seconds} s.\")\n",
    "    print(\"---ğŸŸ¢ Scraping finished. ğŸŸ¢---\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331da8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "table_tennis_stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
