{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b289dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "import random \n",
    "import time \n",
    "import glob\n",
    "import asyncio\n",
    "from typing import Tuple, Optional, List, Dict, Any, Set\n",
    "from pydantic import BaseModel, ValidationError, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b2bc0118",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_EVENTS_DIR = \"../Data/Master/Events\"\n",
    "MASTER_EVENTS_SUFFIX = \"_master_events.csv\"\n",
    "MASTER_EVENTS_REGEX = rf\"^\\d{{8}}{re.escape('_')}{re.escape(MASTER_EVENTS_SUFFIX)}$\"\n",
    "\n",
    "SINGLES_PAYLOADS_DIR = \"../Data/Processed/Singles_match_payloads\"\n",
    "\n",
    "RAW_MATCH_DETAILS_DIR = \"../Data/Raw/Match_details\"\n",
    "os.makedirs(RAW_MATCH_DETAILS_DIR, exist_ok=True)\n",
    "\n",
    "MAX_RETRIES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35d87592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_master_events(master_dir:str, master_regex) -> Tuple[pd.DataFrame,Optional[str]]:\n",
    "    \"\"\"\n",
    "    Parses specified directory for events files in format yyyy_mm_dd. \n",
    "    Attempts to read latest file in this format. \n",
    "\n",
    "    Args:\n",
    "        directory (str): The folder where the master files are stored (e.g., '../Data/Events/Intermediate').\n",
    "        filename_pattern (str): The pattern to match (e.g., '*_events_intermediate.csv').\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame,Optional]: returns DF with data if available or blank df if data unavailable\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(master_dir):\n",
    "        print (f\"‚ùå{master_dir} does not exist as a directory\")\n",
    "        return pd.DataFrame(columns=MINIMAL_EVENT_COLUMNS), None    \n",
    "    \n",
    "    # Get csv files in \n",
    "    files = glob.glob(f\"{master_dir}/*.csv\")\n",
    "   \n",
    "\n",
    "    master_files = []\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"‚ùå No existing *.csv files found in MASTER Events Directory: {master_dir} \")\n",
    "        return pd.DataFrame(columns=MINIMAL_EVENT_COLUMNS), None \n",
    "\n",
    "    for file in files:\n",
    "        filename = os.path.basename(file)    \n",
    "       \n",
    "        if re.match(master_regex,filename):\n",
    "          master_files.append(file)\n",
    "\n",
    "    if not master_files:\n",
    "        print(f\"‚ùå No existing MASTER files in format: {master_regex} in {master_dir}\")\n",
    "        return pd.DataFrame(columns=MINIMAL_EVENT_COLUMNS), None \n",
    "    master_files.sort()    \n",
    "    latest_master = master_files[-1]\n",
    "\n",
    "    try: \n",
    "        latest_master_df = pd.read_csv(latest_master)\n",
    "        print(f\"‚úÖ {len(latest_master_df)} events found in latest MASTER: {latest_master} \")\n",
    "        return latest_master_df, latest_master\n",
    "        \n",
    "    except Exception as e:\n",
    "        print (f\"‚ùå Error reading lastest MASTER, {latest_master}: {e}\")\n",
    "        return pd.DataFrame(columns=MINIMAL_EVENT_COLUMNS), None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2217cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_payloads(singles_payloads_dir:str) -> List[Tuple[int, str]]:\n",
    "    \"\"\"\n",
    "    Reads all singles payload files and returns a complete list of (eventId, documentCode) tuples.\n",
    "    \"\"\"\n",
    "    print(f\"--- üü† Finding  all event ids and payloads from {singles_payloads_dir} ---\")\n",
    "\n",
    "    \n",
    "    all_csv_files = glob.glob(f\"{singles_payloads_dir}/*.csv\")\n",
    "    all_payloads_list = []\n",
    "    \n",
    "    if not all_csv_files:\n",
    "        print(f\"--- ‚ùå ERROR: No CSV files found in {singles_payloads_dir}. ---\")\n",
    "        return []\n",
    "    \n",
    "  \n",
    "    \n",
    "    for file_path in all_csv_files:    \n",
    "        filename = os.path.basename(file_path)       \n",
    "            \n",
    "        \n",
    "        try:\n",
    "            # ONLY get the eventId match code\n",
    "            payload_df = pd.read_csv(file_path, usecols=['eventId', 'documentCode'])\n",
    "            if payload_df.empty:\n",
    "                continue\n",
    "\n",
    "            payload_df['eventId'] = payload_df['eventId'].astype(int)\n",
    "            payload_df['documentCode'] = payload_df['documentCode'].astype(str)            \n",
    "            # Convert to list of (eventId, documentCode) tuples\n",
    "            payloads = list(payload_df[['eventId', 'documentCode']].itertuples(index=False, name=None))\n",
    "            all_payloads_list.extend(payloads)\n",
    "\n",
    "            \n",
    "            if payload_df.empty:\n",
    "                continue\n",
    "        except (pd.errors.EmptyDataError, KeyError, FileNotFoundError) as e:\n",
    "            print(f\"WARN: Could not read payload file {filename}: {e}\")\n",
    "            continue\n",
    "        # check for duplicates and remove :) \n",
    "    all_payloads_list = list(set(all_payloads_list))\n",
    "    if all_payloads_list:\n",
    "        print(f\"--- ‚úÖ All desired payloads: Found {len(all_payloads_list)} total unique matches to scrape. ---\")\n",
    "        return  all_payloads_list\n",
    "    else:\n",
    "        print(f\"‚ùå No Match payloads found\")\n",
    " \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "513ebc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obtained_match_details(raw_match_details_dir:str) -> List[Tuple[int, str]]:\n",
    "    \"\"\"\n",
    "    Parses all obtained singles_match_details files. \n",
    "    Return a list of tuple(eventId, match_code) which are require for scraping\n",
    "    used to determine the matches that are already found. \n",
    "    \"\"\"\n",
    "    print(f\"--- üü† Finding already obtained match details from {raw_match_details_dir} ---\")\n",
    "\n",
    "    all_details_list = []\n",
    "    \n",
    "    # get all file_paths\n",
    "    all_details_files = glob.glob(f\"{raw_match_details_dir}/*match_details.json\")\n",
    "    files_processed_count = 0\n",
    "    \n",
    "    for file_path in all_details_files: \n",
    "        files_processed_count += 1\n",
    "        \n",
    "        # try to read the file (catch error if it fails)\n",
    "        try:\n",
    "            # Safely open and load the JSON file\n",
    "            with open(file_path,\"r\") as f:\n",
    "                matches_list = json.load(f)\n",
    "            \n",
    "            # Check if the file contains the expected list of matches\n",
    "            if not isinstance(matches_list, list):\n",
    "                 print(f\"WARN: Skipping file {os.path.basename(file_path)}: Content is not a list.\")\n",
    "                 continue\n",
    "\n",
    "            # get eventId and match code from each match \n",
    "            for match in matches_list:\n",
    "                event_id_raw = match.get(\"eventId\")\n",
    "                match_code = match.get(\"documentCode\")\n",
    "                \n",
    "                # check the data exists.\n",
    "                if event_id_raw and match_code:\n",
    "                    try:\n",
    "                        event_id = int(event_id_raw) # Ensure ID is an integer\n",
    "                        all_details_list.append((event_id, match_code))\n",
    "                    except ValueError:\n",
    "                        print(f\"WARN: Skipping record in {os.path.basename(file_path)} due to bad data eventId.\")\n",
    "                \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"‚ùå ERROR: Failed to read JSON in {os.path.basename(file_path)}. \")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR: Unexpected error reading {os.path.basename(file_path)}: {type(e).__name__}\")\n",
    "            \n",
    "    # convert to set to remove duplicates \n",
    "    final_unique_list = list(set(all_details_list))\n",
    "    \n",
    "    # get the number of events read \n",
    "    unique_events_obtained = len(set(match_tuple[0] for match_tuple in final_unique_list))\n",
    "    \n",
    "    print(f\"--- ‚úÖ Found {len(final_unique_list)} total unique match details across {files_processed_count} files. ---\")\n",
    "    print(f\"--- ‚úÖ Unique Events with Data: {unique_events_obtained} ---\")\n",
    "\n",
    "    return final_unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20a2e327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 184 events found in latest MASTER: ../Data/Master/Events/20251030__master_events.csv \n",
      "--- üü† Finding  all event ids and payloads from ../Data/Processed/Singles_match_payloads ---\n",
      "--- ‚úÖ All desired payloads: Found 24359 total unique matches to scrape. ---\n",
      "--- üü† Finding already obtained match details from ../Data/Raw/Match_details ---\n",
      "--- ‚úÖ Found 4 total unique match details across 2 files. ---\n",
      "--- ‚úÖ Unique Events with Data: 2 ---\n",
      "\n",
      "üèì Matches to scrape: 24355 across 184 events üèì\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "\n",
    "    latest_master_df, latest_master_file = get_latest_master_events(MASTER_EVENTS_DIR, MASTER_EVENTS_REGEX)\n",
    "    if latest_master_df.empty:\n",
    "        print(f\"‚ùå Exiting: No existing Master Events File available\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    all_payloads = get_all_payloads(SINGLES_PAYLOADS_DIR)\n",
    "    already_obtained_matches = get_obtained_match_details(RAW_MATCH_DETAILS_DIR)\n",
    "\n",
    "    intitial_matches_to_scrape = list(set(all_payloads) - set(already_obtained_matches))\n",
    "    intitial_matches_to_scrape_count = len(intitial_matches_to_scrape)\n",
    "    intitial_events_to_scrape_count = len(set([match_tuple[0] for match_tuple in intitial_matches_to_scrape]))\n",
    "    print(f\"\\nüèì Matches to scrape: {intitial_matches_to_scrape_count} across {intitial_events_to_scrape_count} events üèì\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0886baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def dummy_fetch_detail(match_tuple: Tuple[int, str]) -> Tuple[bool, Optional[Dict[str, Any]], Tuple[int, str], str]:\n",
    "    \"\"\"\n",
    "    Simulates the fetch_match_detail_json API call.\n",
    "    Uses asyncio.sleep to simulate network wait time.\n",
    "    \"\"\"\n",
    "    event_id, match_code = match_tuple\n",
    "    pause_time = random.uniform(1, 3)\n",
    "    \n",
    "    try:\n",
    "        # 1. Wait (Simulate network I/O)\n",
    "        await asyncio.sleep(pause_time)\n",
    "        \n",
    "        # 2. Simulate an example failure\n",
    "        if random.random() < 0.2:\n",
    "            raise TimeoutError(f\"Simulated ReadTimeout for {match_code}\")\n",
    "            \n",
    "        # 3. Return SUCCESS\n",
    "        result_dict = {\n",
    "            \"eventId\": event_id,\n",
    "            \"matchCode\": match_code,\n",
    "            \"simulatedDuration\": round(pause_time, 2),\n",
    "            \"status\": \"OFFICIAL\"\n",
    "        }\n",
    "        status_msg = f\"Simulated time: {round(pause_time, 2)}s.\"\n",
    "        return True, result_dict, match_tuple, status_msg\n",
    "\n",
    "    except Exception as e:\n",
    "        # 4. Return FAILURE\n",
    "        status_msg = f\"Task failed with error: {type(e).__name__}\"\n",
    "        # Return status=False, no data (None), the original input tuple, and the error\n",
    "        return False, None, match_tuple, status_msg\n",
    "\n",
    "\n",
    "\n",
    "async def main_scraper_simulation(tuples_to_scrape: List[Tuple[int, str]]) -> Tuple[List[Dict[str, Any]], List[Tuple[int, str, str]]]:\n",
    "    \"\"\"\n",
    "    Runs the simulation and collects results into success and failure lists.\n",
    "    (Docstring remains the same)\n",
    "    \"\"\"\n",
    "    \n",
    "    successful_matches: List[Dict[str, Any]] = []\n",
    "    # This list will hold the (eventId, matchCode) tuples of failed tasks\n",
    "    failed_matches_log: List[Tuple[int, str, str]] = []\n",
    "    total_tasks = len(tuples_to_scrape)\n",
    "    \n",
    "    # create all coroutines (helper function and the input tuples to scrape (event_id, match_code))\n",
    "    coroutines = [dummy_fetch_detail(task) for task in tuples_to_scrape]\n",
    "\n",
    "    print(f\"--- üöÄ Launching {total_tasks} tasks concurrently... ---\")\n",
    "    start_time = time.time()\n",
    "    processed_count = 0\n",
    "\n",
    "    # process results as coroutines are completed.\n",
    "    for future in asyncio.as_completed(coroutines):\n",
    "        processed_count += 1\n",
    "        \n",
    "        # await result tuple - helper function returns and catches errors itself\n",
    "        # including data validation using pydantic. \n",
    "        # here function returns the input tuple as an output for logging.\n",
    "        status, result_dict, input_tuple, status_msg = await future\n",
    "\n",
    "        # check status of function result\n",
    "        \n",
    "        if status:\n",
    "            # on successfully getting expected api response:\n",
    "            successful_matches.append(result_dict)\n",
    "            \n",
    "            # Logging\n",
    "            event_id, match_code = input_tuple\n",
    "            elapsed = time.time() - start_time\n",
    "            log_line = f\"[{processed_count}/{total_tasks}] ‚úÖ Finished {event_id}:{match_code}. {status_msg} Total elapsed: {elapsed:.2f}s.\"\n",
    "            print(log_line.ljust(80), end='\\r')\n",
    "            \n",
    "        else:\n",
    "            # add the input tuple AND error message to the FAILED log\n",
    "            event_id, match_code = input_tuple\n",
    "            \n",
    "            failed_matches_log.append((event_id, match_code, status_msg))\n",
    "\n",
    "            # print and keep\n",
    "            print(f\"[{processed_count}/{total_tasks}] ‚ùå Task failed {event_id}:{match_code} with error: {status_msg}\".ljust(100))\n",
    "\n",
    "    # Final print summary outside the loop.\n",
    "    print(\" \" * 80, end='\\r') \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"--- üü¢ Simulation Complete ---\")\n",
    "    print(f\"Successfully fetched: {len(successful_matches)} tasks\")\n",
    "    print(f\"Failed to fetch (ready for retry): {len(failed_matches_log)} tasks\")\n",
    "    \n",
    "    return successful_matches, failed_matches_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11c97e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üöÄ Starting initial scrape for 20 matches... ---\n",
      "--- üöÄ Launching 20 tasks concurrently... ---\n",
      "[6/20] ‚ùå Task failed 2932:TTEWSINGLES-----------R64-002900---------- with error: Task failed with error: TimeoutError\n",
      "[14/20] ‚ùå Task failed 2867:TTEWSINGLES-----------QFNL000300---------- with error: Task failed with error: TimeoutError\n",
      "[19/20] ‚ùå Task failed 3108:TTEWSINGLES-----------R128005300---------- with error: Task failed with error: TimeoutError\n",
      "[20/20] ‚ùå Task failed 2693:TTEWSINGLES-----------RND1002300---------- with error: Task failed with error: TimeoutError\n",
      "                                                                                \n",
      "==================================================\n",
      "--- üü¢ Simulation Complete ---\n",
      "Successfully fetched: 16 tasks\n",
      "Failed to fetch (ready for retry): 4 tasks\n",
      "--- Attempt 1 finished in 2.97s. Got 16 new results. ---\n",
      "--- 4 tasks failed and will be retried. ---\n",
      "\n",
      "--- üîÑ Starting Retry 1/1 for 4 remaining matches... ---\n",
      "--- üöÄ Launching 4 tasks concurrently... ---\n",
      "                                                                                : 2.13s. Total elapsed: 2.13s.\n",
      "==================================================\n",
      "--- üü¢ Simulation Complete ---\n",
      "Successfully fetched: 4 tasks\n",
      "Failed to fetch (ready for retry): 0 tasks\n",
      "--- Attempt 2 finished in 2.13s. Got 4 new results. ---\n",
      "\n",
      "==================================================\n",
      "--- üü¢ Simulation Complete. All tasks finished successfully. üü¢---\n",
      "Total successful matches collected: 20\n",
      "Total Wall Clock Time: 5.11 seconds.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Get the initial list of tasks / matches to be scraped\n",
    "    matches_to_scrape = intitial_matches_to_scrape[0:20] # Using your test slice    \n",
    "    # List to hold ALL successful results from all attempts\n",
    "    all_successful_data = []     \n",
    "    retries = 0\n",
    "    global_start_time = time.time() # Start total timer\n",
    "\n",
    "    # Loop as long as we have retries left AND matches to scrape\n",
    "    while (retries < MAX_RETRIES) and bool(matches_to_scrape):\n",
    "        retries += 1 # Increment attempt counter (Attempt 1, 2, ...)\n",
    "        \n",
    "        # --- Logging (As Requested) ---\n",
    "        if retries == 1:\n",
    "            print(f\"--- üöÄ Starting initial scrape for {len(matches_to_scrape)} matches... ---\")\n",
    "        else:\n",
    "            # \\n adds a newline for readability between attempts\n",
    "            print(f\"\\n--- üîÑ Starting Retry {retries-1}/{MAX_RETRIES-1} for {len(matches_to_scrape)} remaining matches... ---\")\n",
    "        \n",
    "        # --- Run the Scrape ---\n",
    "        attempt_start_time = time.time()\n",
    "        \n",
    "        # Run the simulation on the current list of tasks\n",
    "        # successes = list of dicts [{...}, {...}]\n",
    "        # failures = list of tuples [(id, code), (id, code)]\n",
    "        successes, failures = await (main_scraper_simulation(tuples_to_scrape=matches_to_scrape))\n",
    "        \n",
    "        # --- Process Results ---\n",
    "        \n",
    "        # Add new successes to the total collection\n",
    "        all_successful_data.extend(successes)\n",
    "        \n",
    "        # CRITICAL FIX: The list for the *next* iteration is only the tasks that just failed\n",
    "        matches_to_scrape = [(event_id, match_code)for event_id, match_code, error_msg in failures]\n",
    "        # --- Log Results of This Attempt ---\n",
    "        attempt_duration = time.time() - attempt_start_time\n",
    "        if successes: # Only print if some were found\n",
    "            print(f\"--- Attempt {retries} finished in {attempt_duration:.2f}s. Got {len(successes)} new results. ---\")\n",
    "        if failures: # Only print if some remain\n",
    "            print(f\"--- {len(failures)} tasks failed and will be retried. ---\")\n",
    "\n",
    "\n",
    "    # --- Final Summary (Outside the loop) ---\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    \n",
    "    if not matches_to_scrape: # If the final 'failures' list is empty\n",
    "        print(\"--- üü¢ Simulation Complete. All tasks finished successfully. üü¢---\")\n",
    "    else:\n",
    "        print(f\"--- ‚ö†Ô∏è Simulation Complete. {len(matches_to_scrape)} tasks permanently failed after {MAX_RETRIES} attempts. ---\")\n",
    "        \n",
    "    print(f\"Total successful matches collected: {len(all_successful_data)}\")\n",
    "    total_duration = time.time() - global_start_time\n",
    "    print(f\"Total Wall Clock Time: {total_duration:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "977d21a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc1c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b7bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "table_tennis_stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
