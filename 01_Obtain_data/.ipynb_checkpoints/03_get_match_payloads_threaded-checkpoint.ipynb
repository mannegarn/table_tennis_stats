{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5f52b33-4dd4-4609-9c6a-6ef248415876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sys\n",
    "import concurrent.futures \n",
    "from typing import Union, Dict, List, Tuple, Any\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a6c1058-028a-4382-9e9d-ffbdaa5f5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script fetches the raw JSON containing all \"match payloads\" containing match metadata\n",
    "    for a given WTT event ID using a GET request.\n",
    "    \n",
    "    Match-codes contained in payload are reequired for subsequent API call to get full match details.\n",
    "    \n",
    "    Reverse engineered from WTT events pages such as:\n",
    "    https://www.worldtabletennis.com/eventInfo?eventId=3085&selectedTab=Matches\n",
    "\n",
    "    Events_file is a csv containing the events list of events to be scraped based on their unique event ID.\n",
    "\n",
    "    A csv file is made for each event containing all match payloads for that event.\n",
    "\n",
    "    Threading has been implemented to speed up the proccess.\n",
    "    \n",
    "\"\"\"\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# Specifying the csv containing all the events from\n",
    "EVENTS_FILE = \"../Data/Processed/Events/shortlist_events.csv\"\n",
    "\n",
    "# A csv for each event containing its match payloads will be saved to this directory/\n",
    "OUTPUT_DIR = \"../Data/Raw/Match_payloadst\"\n",
    "\n",
    "# Values used to generate random pause duration in seconds for API politeness\n",
    "MIN_PAUSE = 0.1 \n",
    "MAX_PAUSE = 0.2 \n",
    "\n",
    "# Number of threads for the IO processing.\n",
    "# Based on reading - 20 is a good starting number:\n",
    "MAX_WORKERS = 20 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3776bfc0-dfdb-47e6-aa40-da7173780d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function used by each worker\n",
    "# API call as well as file saving occurs inside the fucntion.\n",
    "\n",
    "def fetch_and_save_payload(event_id: Union[int, str], output_dir: str, min_pause: float, max_pause: float) -> Tuple[int, bool, int, str]:\n",
    "    \"\"\"\n",
    "    For one event: fetches match payloads, saves to CSV, and handles errors and reporting.\n",
    "    For use with threadpool workers\n",
    "    Returns: (event_id, status_bool, match_count, status_message)\n",
    "    \"\"\"\n",
    "\n",
    "    # Define API endpoint URL and necessary params + headers.\n",
    "    url = \"https://liveeventsapi.worldtabletennis.com/api/cms/GetOfficialResult\"\n",
    "    params = {'EventId': str(event_id), \"DocumentCode\": \"TTE\"}\n",
    "    headers = {\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Referer': 'https://www.worldtabletennis.com/',\n",
    "        'User-Agent': 'Mozilla/5.0 (Linux; Android 11.0; Surface Duo) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Mobile Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # initialise variables for match_count and the status message for logging.\n",
    "    match_count = 0\n",
    "    status_msg = \"\"\n",
    "\n",
    "    # Try the api call and get response as json. Timeout set to keep thread running if API TimeoutError occurs\n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers, timeout=15)\n",
    "        \n",
    "        # Raise exception for bad status codes (4xx request errors or 5xx server errors)\n",
    "        response.raise_for_status()\n",
    "        raw_payloads = response.json() \n",
    "\n",
    "        # Check if response is a list as expected.\n",
    "        if not isinstance(raw_payloads, list):\n",
    "            # return content for logging, type.__name__ isolates typename as string  \n",
    "            status_msg = f\"JSON was not a list ({type(raw_payloads).__name__})\"\n",
    "            # return content for logginga\n",
    "            return event_id, False, 0, status_msg\n",
    "\n",
    "        # convert match payloads to df and get length / number of matches\n",
    "        payloads_df = pd.DataFrame(raw_payloads)\n",
    "        match_count = len(payloads_df)\n",
    "\n",
    "        # Create filename and save df to csv (even if content is blank)\n",
    "        filename = os.path.join(OUTPUT_DIR,f\"{event_id}_match_payloads.csv\")\n",
    "        payloads_df.to_csv(filename, index=False)\n",
    "\n",
    "        sleep_duration = random.uniform(min_pause, max_pause)\n",
    "        time.sleep(sleep_duration)\n",
    "\n",
    "        return event_id, True, match_count, f\"Found {match_count} matches for event:{event_id}. Pausing for {sleep_duration:.1f}s.\"\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        status_msg = f\"HTTP Error: {e.response.status_code}\"\n",
    "    except Exception as e:\n",
    "        status_msg = f\"Error: {type(e).__name__}\"\n",
    "\n",
    "    return event_id, False, 0, status_msg \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2f3ec43-49ff-425b-a5ea-0f9489c64c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_events_to_scrape(shortlist_df: pd.DataFrame, output_dir: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Checks output directory against the event_shortlist to check and return event_ids where data is not yet found or where \n",
    "    the event is not yet complete (as more matches can be added)\n",
    "    \n",
    "    Args:\n",
    "        shortlist_df (pd.DataFrame): Shortlist of evente to be scraped\n",
    "        output_dir (str): The directory where existing match payloads are saved.\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: A series of event_ids that need to be scraped for more data (availability of data will be checked later)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n--- 🟠 Starting Check on {len(os.listdir(output_dir))} existing files in {output_dir}... 🟠 ---\")\n",
    "    \n",
    "    # Use the input DF length for initial count\n",
    "    events_total_initial = len(shortlist_df)\n",
    "    \n",
    "    # initialise variables\n",
    "    already_obtained_events: Set[int] = set()\n",
    "    files_checked_count = 0 \n",
    "    \n",
    "  \n",
    "    \n",
    "    # Parse the output directory for existing match payloads files\n",
    "    existing_files = glob.glob(f\"{output_dir}/*.csv\")\n",
    "    \n",
    "    \n",
    "    # Loop through the existing files; check if data is there, and check if event is still ongoing.\n",
    "    \n",
    "    for file_path in existing_files: \n",
    "         # Initialise variables for logging\n",
    "        files_checked_count += 1\n",
    "        current_event_id = None\n",
    "\n",
    "        # First Try Except to handle IO errors such as bad files.\n",
    "        try:\n",
    "            # create of payloads for current event \n",
    "            existing_payloads_df = pd.read_csv(file_path)\n",
    "            \n",
    "            # If no data, then continue.\n",
    "            if existing_payloads_df.empty:\n",
    "                continue\n",
    "\n",
    "            # Inner Try except catches (KeyError, IndexError)\n",
    "            try:\n",
    "                # get event id from the payloads df (rather than checking filename)\n",
    "                current_event_id = existing_payloads_df[\"eventId\"].iloc[0]\n",
    "                \n",
    "                # get the event listing from the events_shortlist that contains complete evnent metadata\n",
    "                event_entry = shortlist_df[shortlist_df[\"eventId\"] == current_event_id]\n",
    "\n",
    "                # if eventry entry is not found in shortlist, skip to next file in loop.\n",
    "                if event_entry.empty:\n",
    "                    continue\n",
    "\n",
    "                # get completed status (True or False)\n",
    "                event_completed = event_entry[\"Completed\"].iloc[0]\n",
    "                \n",
    "                # Only add to skip list if the event is completed! \n",
    "                if event_completed:\n",
    "                    already_obtained_events.add(current_event_id)\n",
    "                    \n",
    "            except (KeyError, IndexError, pd.errors.EmptyDataError) as err:\n",
    "                # Prints errors for index problems missing data \n",
    "                print(f\" Skipping file: {os.path.basename(file_path)}. Data error: {type(err).__name__}\")\n",
    "                continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Catches other errors if they occur.\n",
    "            #print(f\"ERROR reading file {os.path.basename(file_path)}: {type(e).__name__}\")\n",
    "            continue\n",
    "\n",
    "    \n",
    "\n",
    "    # Filter df to get event ids required to scrape\n",
    "    \n",
    "\n",
    "   # mask to get events that have already been obtained\n",
    "    event_ids_series = shortlist_df[\"eventId\"].astype(int)\n",
    "    mask = shortlist_df[\"eventId\"].isin(already_obtained_events)\n",
    "    # filters out events that are obtained already - leaves only event id that need to be scraped\n",
    "    events_to_scrape_df = shortlist_df[~mask].copy()\n",
    "\n",
    "    #count for final print statement\n",
    "    events_to_scrape_count = len(events_to_scrape_df)\n",
    "\n",
    "   \n",
    "    print(f\"\\n--- CHECK COMPLETE: {events_to_scrape_count}/{events_total_initial} events remaining to scrape. ---\")\n",
    "    \n",
    "    print(f\"✅ Total Events: {events_total_initial} | Files Checked: {files_checked_count} | Already Obtained: {len(already_obtained_events)} | To Scrape: {events_to_scrape_count}\")\n",
    "    \n",
    "    # Return the clean Series of IDs\n",
    "    return events_to_scrape_df[\"eventId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db6801e-6bb6-4163-902e-e3f50d3ec80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---🚀 Starting Obtaining Match Payloads 🚀---\n",
      "\n",
      "--- 🟠 Starting Check on 74 existing files in ../Data/Raw/Match_payloadst... 🟠 ---\n",
      "\n",
      "--- CHECK COMPLETE: 281/302 events remaining to scrape. ---\n",
      "✅ Total Events: 302 | Files Checked: 74 | Already Obtained: 21 | To Scrape: 281\n",
      "\n",
      "---🚀 Starting Concurrent Scraping for 281 Events 🚀---\n",
      "--- Using 20 threads. API pause: 0.1s - 0.2s ---\n",
      "Event 2099: (3/281)  ✅ Found 0 matches for event:2099. Pausing for 0.2s.                            "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    start_time = time.time() # Start timing the entire run\n",
    "    print(\"---🚀 Starting Obtaining Match Payloads 🚀---\")\n",
    "\n",
    "    # 1. Load Shortlist DF and Perform Skip Check\n",
    "    try:\n",
    "        shortlist_df_object = pd.read_csv(EVENTS_FILE)\n",
    "        # Ensure 'Completed' column is boolean after loading\n",
    "        shortlist_df_object['Completed'] = shortlist_df_object['Completed'].astype(bool)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"--- ❌ ERROR: Shortlist file not found at {EVENTS_FILE}. ---\")\n",
    "        sys.exit(1)\n",
    "    except KeyError as e:\n",
    "        print(f\"--- ❌ ERROR: Necessary column '{e}' not found in {EVENTS_FILE}. ---\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # Call the filter function to get the Series of IDs to actually scrape\n",
    "    events_to_scrape_ids = filter_events_to_scrape(shortlist_df_object, OUTPUT_DIR)\n",
    "    # Convert Series to Python list for the executor\n",
    "    event_ids_to_process = events_to_scrape_ids.tolist()\n",
    "    events_to_scrape_count = len(event_ids_to_process)\n",
    "\n",
    "    # Exit cleanly if no events need scraping after the skip check\n",
    "    if events_to_scrape_count == 0:\n",
    "        print(\"\\n--- ✅ PROCESS COMPLETE: No events remaining to scrape. ---\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    # --- Counters & Setup for Parallel Execution ---\n",
    "    processed_count = 0\n",
    "    successful_count = 0\n",
    "    failed_event_ids: List[int] = []\n",
    "    start_time_loop = time.time() # Start timer specifically for the concurrent loop\n",
    "\n",
    "    print(f\"\\n---🚀 Starting Concurrent Scraping for {events_to_scrape_count} Events 🚀---\")\n",
    "    print(f\"--- Using {MAX_WORKERS} threads. API pause: {MIN_PAUSE:.1f}s - {MAX_PAUSE:.1f}s ---\")\n",
    "\n",
    "    #  Launch paralllel execution\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "\n",
    "        # Submit all tasks immediately.\n",
    "        # Map the 'Future' object back to the event_id for easy lookup.\n",
    "        futures = {\n",
    "            executor.submit(fetch_and_save_payload, event_id, OUTPUT_DIR, MIN_PAUSE, MAX_PAUSE): event_id\n",
    "            for event_id in event_ids_to_process\n",
    "        }\n",
    "\n",
    "        # Process Results Asynchronously (as they complete)\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "\n",
    "            processed_count += 1\n",
    "            event_id = futures[future] # Get the event_id for this completed future\n",
    "\n",
    "            try:\n",
    "                # Retrieve the result tuple: (event_id, status_bool, match_count, status_message)\n",
    "                result_id, status, match_count, status_msg = future.result()\n",
    "\n",
    "                # --- In-Place Status Line (Overwrites itself) ---\n",
    "                if status:\n",
    "                    successful_count += 1                   \n",
    "                    # Construct the status message\n",
    "                    log_line = f\"Event {event_id}: ({processed_count}/{events_to_scrape_count})  ✅ {status_msg}\"\n",
    "                else:\n",
    "                   \n",
    "                    failed_event_ids.append(event_id)\n",
    "                    # Construct the failure message\n",
    "                    log_line = f\"Event {event_id}: ({processed_count}/{events_to_scrape_count})  ❌ Failed: {status_msg}\"\n",
    "\n",
    "                # Print the status line in place, padded to clear previous content\n",
    "                print(log_line.ljust(100), end='\\r') # Increased padding slightly\n",
    "\n",
    "                # --- Checkpoint Log (Every 10 events, prints a NEW LINE) ---\n",
    "                if processed_count % 10 == 0:\n",
    "                    elapsed_time = time.time() - start_time_loop\n",
    "                    minutes = int(elapsed_time // 60)\n",
    "                    seconds = int(elapsed_time % 60)\n",
    "                    # Print checkpoint on a new line for clarity\n",
    "                    print(f\"\\n--- {processed_count}/{events_to_scrape_count} processed. Success Rate: {successful_count/processed_count:.1%}. Elapsed: {minutes}m {seconds}s ---\")\n",
    "                    # After checkpoint, immediately reprint the current status in-place to avoid blank line\n",
    "                    print(log_line.ljust(100), end='\\r')\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                # Catches unexpected errors *during result retrieval*\n",
    "                # Print fatal error on a new line, padded\n",
    "                print(f\"\\n--- ❌ FATAL ERROR processing result for Event {event_id}: {type(e).__name__} ---\".ljust(100))\n",
    "                failed_event_ids.append(event_id)\n",
    "\n",
    "    # --- End of Loop ---\n",
    "    # Final cleanup print to clear the last in-place status line before the summary\n",
    "    print(\" \" * 100, end='\\r')\n",
    "\n",
    "    # 3. Final Summary\n",
    "    total_run_time = time.time() - start_time\n",
    "    total_minutes = int(total_run_time // 60)\n",
    "    total_seconds = int(total_run_time % 60)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"✅ Finished! Match payloads obtained for {successful_count}/{events_to_scrape_count} requested events.\")\n",
    "    if failed_event_ids:\n",
    "        print(f\"⚠️ Failed Event IDs ({len(failed_event_ids)}): {failed_event_ids}\")\n",
    "    print(f\"Total run time = {total_minutes} m and {total_seconds} s. (Parallel Mode)\")\n",
    "    print(\"---🟢 Scraping finished. 🟢---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dfd599-9d19-42b2-944f-9f1e1c1299f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
