{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b289dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "import random \n",
    "import time \n",
    "import glob\n",
    "import asyncio\n",
    "from typing import Tuple, Optional, List, Dict, Any, Set\n",
    "from pydantic import BaseModel, ValidationError, Field\n",
    "import aiohttp\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2bc0118",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_EVENTS_DIR = \"../Data/Master/Events\"\n",
    "MASTER_EVENTS_SUFFIX = \"_master_events.csv\"\n",
    "MASTER_EVENTS_REGEX = rf\"^\\d{{8}}{re.escape('_')}{re.escape(MASTER_EVENTS_SUFFIX)}$\"\n",
    "\n",
    "SINGLES_PAYLOADS_DIR = \"../Data/Processed/Singles_match_payloads\"\n",
    "os.makedirs(SINGLES_PAYLOADS_DIR, exist_ok=True)\n",
    "\n",
    "RAW_MATCH_DETAILS_DIR = \"../Data/Raw/Match_details\"\n",
    "os.makedirs(RAW_MATCH_DETAILS_DIR, exist_ok=True)\n",
    "\n",
    "MIN_PAUSE = 0.00\n",
    "MAX_PAUSE = 0.02\n",
    "\n",
    "MAX_RETRIES = 4\n",
    "\n",
    "FAILURE_LOG_PATH = \"../Data/Raw/failure_log_match_details.csv\"\n",
    "\n",
    "\n",
    "# only used in get_latest_master_events to return blank df if no files found and further checks are required\n",
    "MINIMAL_EVENT_COLUMNS = [\"eventId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81958106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic models for basic validation for match details api response.\n",
    "\n",
    "# contains two player models inside a list with key = competitiors (THERE IS A TYPO IN THE API)\n",
    "class CompetitorsModel(BaseModel):\n",
    "    \"\"\"Ensures the 'players' list (containing PlayerModel) exists.\"\"\"\n",
    "    competitiorId: str\n",
    " \n",
    "\n",
    "class MatchDetailModel(BaseModel):\n",
    "    \"\"\"Ensures the top-level keys we need for filtering and analysis exist.\"\"\"\n",
    "    eventId: str\n",
    "    documentCode: str\n",
    "    #This field can be null\n",
    "    resultOverallScores: Optional[str] = None \n",
    "    # Must be a list of CompetitorModels\n",
    "    # Mispelling of competitiors is intentional to match the api response\n",
    "    competitiors: List[CompetitorsModel] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d87592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_master_events(master_dir:str, master_regex) -> Tuple[pd.DataFrame,Optional[str]]:\n",
    "    \"\"\"\n",
    "    Parses specified directory for events files in format yyyy_mm_dd. \n",
    "    Attempts to read latest file in this format. \n",
    "\n",
    "    Args:\n",
    "        directory (str): The folder where the master files are stored (e.g., '../Data/Events/Intermediate').\n",
    "        filename_pattern (str): The pattern to match (e.g., '*_events_intermediate.csv').\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame,Optional]: returns DF with data if available or blank df if data unavailable\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(master_dir):\n",
    "        print (f\"‚ùå{master_dir} does not exist as a directory\")\n",
    "        return pd.DataFrame(columns=MINIMAL_EVENT_COLUMNS), None    \n",
    "    \n",
    "    # Get csv files in \n",
    "    files = glob.glob(f\"{master_dir}/*.csv\")\n",
    "   \n",
    "\n",
    "    master_files = []\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"‚ùå No existing *.csv files found in MASTER Events Directory: {master_dir} \")\n",
    "        return pd.DataFrame(columns=MINIMAL_EVENT_COLUMNS), None \n",
    "\n",
    "    for file in files:\n",
    "        filename = os.path.basename(file)    \n",
    "       \n",
    "        if re.match(master_regex,filename):\n",
    "          master_files.append(file)\n",
    "\n",
    "    if not master_files:\n",
    "        print(f\"‚ùå No existing MASTER files in format: {master_regex} in {master_dir}\")\n",
    "        return pd.DataFrame(columns=MINIMAL_EVENT_COLUMNS), None \n",
    "    master_files.sort()    \n",
    "    latest_master = master_files[-1]\n",
    "\n",
    "    try: \n",
    "        latest_master_df = pd.read_csv(latest_master)\n",
    "        print(f\"‚úÖ {len(latest_master_df)} events found in latest MASTER: {latest_master} \")\n",
    "        return latest_master_df, latest_master\n",
    "        \n",
    "    except Exception as e:\n",
    "        print (f\"‚ùå Error reading lastest MASTER, {latest_master}: {e}\")\n",
    "        return pd.DataFrame(columns=MINIMAL_EVENT_COLUMNS), None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2217cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_payloads(singles_payloads_dir:str) -> List[Tuple[int, str]]:\n",
    "    \"\"\"\n",
    "    Reads all singles payload files and returns a complete list of (eventId, documentCode) tuples.\n",
    "    \"\"\"\n",
    "    print(f\"--- üü† Finding  all event ids and payloads from {singles_payloads_dir} ---\")\n",
    "\n",
    "    \n",
    "    all_csv_files = glob.glob(f\"{singles_payloads_dir}/*.csv\")\n",
    "    all_payloads_list = []\n",
    "    \n",
    "    if not all_csv_files:\n",
    "        print(f\"--- ‚ùå ERROR: No CSV files found in {singles_payloads_dir}. ---\")\n",
    "        return []\n",
    "    \n",
    "  \n",
    "    \n",
    "    for file_path in all_csv_files:    \n",
    "        filename = os.path.basename(file_path)       \n",
    "            \n",
    "        \n",
    "        try:\n",
    "            # ONLY get the eventId match code\n",
    "            payload_df = pd.read_csv(file_path, usecols=['eventId', 'documentCode'])\n",
    "            if payload_df.empty:\n",
    "                continue\n",
    "\n",
    "            payload_df['eventId'] = payload_df['eventId'].astype(int)\n",
    "            payload_df['documentCode'] = payload_df['documentCode'].astype(str)            \n",
    "            # Convert to list of (eventId, documentCode) tuples\n",
    "            payloads = list(payload_df[['eventId', 'documentCode']].itertuples(index=False, name=None))\n",
    "            all_payloads_list.extend(payloads)\n",
    "\n",
    "            \n",
    "            if payload_df.empty:\n",
    "                continue\n",
    "        except (pd.errors.EmptyDataError, KeyError, FileNotFoundError) as e:\n",
    "            print(f\"--- ‚ùåERROR: Could not read payload file {filename}: {e}\")\n",
    "            continue\n",
    "        # check for duplicates and remove :) \n",
    "    all_payloads_list = list(set(all_payloads_list))\n",
    "    if all_payloads_list:\n",
    "        print(f\"--- ‚úÖ All desired payloads: Found {len(all_payloads_list)} total unique matches to scrape. ---\")\n",
    "        return  all_payloads_list\n",
    "    else:\n",
    "        print(f\"‚ùå No Match payloads found\")\n",
    " \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "513ebc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obtained_match_details(raw_match_details_dir:str) -> List[Tuple[int, str]]:\n",
    "    \"\"\"\n",
    "    Parses all obtained singles_match_details files. \n",
    "    Return a list of tuple(eventId, match_code) which are require for scraping\n",
    "    used to determine the matches that are already found. \n",
    "    \"\"\"\n",
    "    print(f\"--- üü† Finding already obtained match details from {raw_match_details_dir} ---\")\n",
    "\n",
    "    all_details_list = []\n",
    "    \n",
    "    # get all file_paths\n",
    "    all_details_files = glob.glob(f\"{raw_match_details_dir}/*match_details.json\")\n",
    "    files_processed_count = 0\n",
    "    \n",
    "    for file_path in all_details_files: \n",
    "        files_processed_count += 1\n",
    "        \n",
    "        # try to read the file (catch error if it fails)\n",
    "        try:\n",
    "            # Safely open and load the JSON file\n",
    "            with open(file_path,\"r\") as f:\n",
    "                matches_list = json.load(f)\n",
    "            \n",
    "            # Check if the file contains the expected list of matches\n",
    "            if not isinstance(matches_list, list):\n",
    "                 print(f\" Skipping file {os.path.basename(file_path)}: Content is not a list.\")\n",
    "                 continue\n",
    "\n",
    "            # get eventId and match code from each match \n",
    "            for match in matches_list:\n",
    "                event_id_raw = match.get(\"eventId\")\n",
    "                match_code = match.get(\"documentCode\")\n",
    "                \n",
    "                # check the data exists.\n",
    "                if event_id_raw and match_code:\n",
    "                    try:\n",
    "                        event_id = int(event_id_raw) # Ensure ID is an integer\n",
    "                        all_details_list.append((event_id, match_code))\n",
    "                    except ValueError:\n",
    "                        print(f\"Skipping record in {os.path.basename(file_path)} due to bad data eventId.\")\n",
    "                \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"‚ùå ERROR: Failed to read JSON in {os.path.basename(file_path)}. \")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR: Unexpected error reading {os.path.basename(file_path)}: {type(e).__name__}\")\n",
    "            \n",
    "    # convert to set to remove duplicates \n",
    "    final_unique_list = list(set(all_details_list))\n",
    "    \n",
    "    # get the number of events read \n",
    "    unique_events_obtained = len(set(match_tuple[0] for match_tuple in final_unique_list))\n",
    "    \n",
    "    print(f\"--- ‚úÖ Found {len(final_unique_list)} total unique match details across {files_processed_count} files. ---\")\n",
    "    print(f\"--- ‚úÖ Unique Events with Data: {unique_events_obtained} ---\")\n",
    "\n",
    "    return final_unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa5cc26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_match_details(session: aiohttp.ClientSession, \n",
    "                            match_tuple: Tuple[int, str], \n",
    "                            min_pause: float, \n",
    "                            max_pause: float\n",
    "                           ) -> Tuple[bool, Optional[Dict[str, Any]], Tuple[int, str], str]:\n",
    "    \"\"\"\n",
    "    Unpacks match_tuple to get event_id and match_code used to call API endpoint.\n",
    "    Returns:\n",
    "        status: A boolean indicating success (True) or failure (False).\n",
    "        dict: The FULL RAW JSON response as a Python dictionary, or None if an error occurs.\n",
    "        match_tuple: The original input tuple (event_id, match_code).\n",
    "        status_msg: A string message to log error codes upon failure.\n",
    "    \"\"\"\n",
    "    # pause before running for api politeness\n",
    "    await asyncio.sleep(random.uniform(min_pause,max_pause)) \n",
    "            \n",
    "            \n",
    "    event_id, match_code = match_tuple\n",
    "    status_msg = \"\"\n",
    "\n",
    "\n",
    "    url = f\"https://liveeventsapi.worldtabletennis.com/api/cms/GetMatchCardDetails/{event_id}/{match_code}?&use_live_match_cache=false\"\n",
    "    \n",
    "    headers = {\n",
    "        'Accept': 'application/json',\n",
    "        'Accept-Language': 'en-GB,en;q=0.9,es;q=0.8',\n",
    "        'Cache-Control': 'no-cache',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Content-Type': 'application/json',\n",
    "        'DNT': '1',\n",
    "        'Origin': 'https://www.worldtabletennis.com',\n",
    "        'Pragma': 'no-cache',\n",
    "        'Referer': 'https://www.worldtabletennis.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-site',\n",
    "        'User-Agent': 'Mozilla/5.0 (Linux; Android 11.0; Surface Duo) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Mobile Safari/537.36',\n",
    "        'sec-ch-ua': '\"Chromium\";v=\"140\", \"Not=A?Brand\";v=\"24\", \"Google Chrome\";v=\"140\"',\n",
    "        'sec-ch-ua-mobile': '?1',\n",
    "        'sec-ch-ua-platform': '\"Android\"'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        async with session.get(url,headers=headers,timeout = 20) as response:\n",
    "            \n",
    "            # Raise an error for bad status codes\n",
    "            response.raise_for_status()       \n",
    "            raw_json_response = await response.json()                \n",
    "\n",
    "            if raw_json_response:          \n",
    "               \n",
    "                # validate and dump\n",
    "                # raises error if validation fails.\n",
    "                MatchDetailModel.model_validate(raw_json_response)\n",
    "\n",
    "                status_msg = f\"Successful, raw JSON response for {match_tuple}.\"\n",
    "                return True, raw_json_response, match_tuple, status_msg \n",
    "               \n",
    "            else:\n",
    "                status_msg = f\"No data returned for {match_tuple}.\"\n",
    "                return False, None, match_tuple, status_msg  \n",
    "    except ValidationError as e: # Catches Pydantic schema errors\n",
    "        try:\n",
    "            first_error = e.errors()[0]\n",
    "            field = \".\".join(map(str, first_error['loc']))\n",
    "            status_msg = f\"Task failed with error: Validation Error (Field '{field}' missing)\"\n",
    "        except:\n",
    "             status_msg = \"Task failed with error: ValidationError (Unknown structure)\"\n",
    "        return False, None, match_tuple, status_msg      \n",
    "\n",
    "    except Exception as e: \n",
    "        status_msg = f\"Error: {type(e).__name__}\"\n",
    "        return False, None, match_tuple, status_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0886baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main_scraper(tuples_to_scrape: List[Tuple[int, str]]) -> Tuple[List[Dict[str, Any]], List[Tuple[int, str, str]]]:\n",
    "    \"\"\"\n",
    "    Runs the simulation and collects results into success and failure lists.\n",
    "    (Docstring remains the same)\n",
    "    \"\"\"\n",
    "    \n",
    "    successful_matches: List[Dict[str, Any]] = []\n",
    "    # This list will hold the (eventId, matchCode) tuples of failed tasks\n",
    "    failed_matches_log: List[Tuple[int, str, str]] = []\n",
    "    total_tasks = len(tuples_to_scrape)\n",
    "    \n",
    "    #start session for concurrent requests\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "    \n",
    "        # create all coroutines (helper function and the input tuples to scrape (event_id, match_code))\n",
    "        coroutines = [get_match_details(session, task, MIN_PAUSE, MAX_PAUSE) for task in tuples_to_scrape]\n",
    "\n",
    "        print(f\"--- üöÄ Launching {total_tasks} tasks concurrently... ---\")\n",
    "        start_time = time.time()\n",
    "        processed_count = 0\n",
    "\n",
    "        # process results as coroutines are completed.\n",
    "        for future in asyncio.as_completed(coroutines):\n",
    "            processed_count += 1\n",
    "            \n",
    "            # await result tuple - helper function returns and catches errors itself\n",
    "            # including data validation using pydantic. \n",
    "            # here function returns the input tuple as an output for logging.\n",
    "            status, result_dict, input_tuple, status_msg = await future\n",
    "\n",
    "            # check status of function result\n",
    "            \n",
    "            if status:\n",
    "                # on successfully getting expected api response:\n",
    "                successful_matches.append(result_dict)\n",
    "                \n",
    "                # logging\n",
    "                event_id, match_code = input_tuple\n",
    "                elapsed = time.time() - start_time                \n",
    "                \n",
    "            else:\n",
    "                # add the input tuple AND error message to the FAILED log\n",
    "                event_id, match_code = input_tuple                \n",
    "                failed_matches_log.append((event_id, match_code, status_msg))\n",
    "            # print every 200 tasks to keep track of progress\n",
    "            if processed_count % 20 == 0:\n",
    "                log_line = f\"--- üü† [{processed_count}/{total_tasks}] Time elapsed: {elapsed:.2f}s. üü† ---\"\n",
    "                print(log_line.ljust(80), end='\\r')\n",
    "                # print every 10 tasks to keep track of pro\n",
    "\n",
    "        # Final print summary outside the loop.\n",
    "        print(\" \" * 80, end='\\r') \n",
    "        print(\"\\n\" + \"=\" * 50)        \n",
    "        print(f\"üü¢ Successfully fetched: {len(successful_matches)} tasks üü¢\")\n",
    "        print(f\"Failed to fetch (ready for retry): {len(failed_matches_log)} tasks\")\n",
    "        \n",
    "        return successful_matches, failed_matches_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f38044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_match_details(all_match_details: List[Dict[str, Any]]) -> Dict[int, List[Dict[str, Any]]]:\n",
    "    \"\"\"Groups all fetched match details into a dictionary where the key is the eventId.\"\"\"\n",
    "    event_groups: Dict[int, List[Dict[str, Any]]] = {}\n",
    "\n",
    "    for match_detail in all_match_details:\n",
    "        \n",
    "        event_id_raw = match_detail.get('eventId')\n",
    "        if event_id_raw is not None:\n",
    "            try:\n",
    "                event_id = int(event_id_raw)\n",
    "                event_groups.setdefault(event_id, []).append(match_detail)\n",
    "            except ValueError:\n",
    "                print(f\"WARN: Skipping record due to non-integer Event ID: {event_id_raw}\")\n",
    "    return event_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edda540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_json_list(output_filename: str, new_matches: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Reads a list from a JSON file, appends new match data, and writes the full list back.\n",
    "    If the file doesn't exist, it creates a new file with the data.\n",
    "    \"\"\"\n",
    "    existing_list = []\n",
    "\n",
    "    # READ and Check if the file exists and read its contents.\n",
    "    if os.path.exists(output_filename) and os.path.getsize(output_filename) > 0:\n",
    "        try:\n",
    "            with open(output_filename, 'r') as f:\n",
    "                # Load the current list from the file\n",
    "                existing_list = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"‚ùå Warning: Could not decode JSON from {output_filename}. Starting with an empty list.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading {output_filename}: {e}. Starting with an empty list.\")\n",
    "\n",
    "    # Ensure existing_list is actually a list before appending new data \n",
    "    if not isinstance(existing_list, list):\n",
    "        print(f\"‚ùå Warning: Data in {output_filename} was not a list. Overwriting with new data.\")\n",
    "        existing_list = []\n",
    "    \n",
    "   \n",
    "    existing_ids = {match.get('id') for match in existing_list if isinstance(match, dict)}\n",
    "    added_count = 0\n",
    "    for match in new_matches:\n",
    "        if isinstance(match, dict) and match.get('id') not in existing_ids:\n",
    "            existing_list.append(match)\n",
    "            added_count += 1\n",
    "  \n",
    "    try:\n",
    "        with open(output_filename, 'w') as f:\n",
    "            # use W mode as we are writing a new, COMPLETE, updated set of data.\n",
    "            json.dump(existing_list, f, indent=4)        \n",
    "        \n",
    "        # Use  end='\\r' return to create a dynamic updating line\n",
    "        print(f\"‚úÖ Updated Event {os.path.basename(output_filename)}: Added {added_count} new matches (Total: {len(existing_list)}).\", end='\\r')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error writing to {output_filename}: {e}\".ljust(80)) \n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b60e57f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 187 events found in latest MASTER: ../Data/Master/Events/20251110__master_events.csv \n",
      "--- üü† Finding  all event ids and payloads from ../Data/Processed/Singles_match_payloads ---\n",
      "--- ‚úÖ All desired payloads: Found 24665 total unique matches to scrape. ---\n",
      "--- üü† Finding already obtained match details from ../Data/Raw/Match_details ---\n",
      "--- ‚úÖ Found 24665 total unique match details across 187 files. ---\n",
      "--- ‚úÖ Unique Events with Data: 187 ---\n",
      "\n",
      "üèì Matches to scrape: 0 across 0 events üèì\n",
      "\n",
      "==================================================\n",
      "--- ‚úÖüèìüü¢ Scraping Complete. All tasks finished successfully. üü¢üèì‚úÖ---\n",
      "Total successful matches collected: 0\n",
      "Total Wall Clock Time: 0.00 seconds.\n",
      "--- üíæ Saving data to disk (Read-Modify-Write) ---\n",
      "\n",
      "--- ‚úÖ All successful events written to disk. (0 files) ---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    \n",
    "    # Get the latest master events file\n",
    "\n",
    "    latest_master_df, latest_master_file = get_latest_master_events(MASTER_EVENTS_DIR, MASTER_EVENTS_REGEX)\n",
    "    if latest_master_df.empty:\n",
    "        print(f\"‚ùå Exiting: No existing Master Events File available\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # get all payloads that could be scraped for match details\n",
    "    # compare this to matches already obtained, leaving only payloads left to be scraped!Q\n",
    "\n",
    "    all_payloads = get_all_payloads(SINGLES_PAYLOADS_DIR)\n",
    "    already_obtained_matches = get_obtained_match_details(RAW_MATCH_DETAILS_DIR)\n",
    "\n",
    "    intitial_matches_to_scrape = list(set(all_payloads) - set(already_obtained_matches))\n",
    "    intitial_matches_to_scrape_count = len(intitial_matches_to_scrape)\n",
    "    intitial_events_to_scrape_count = len(set([match_tuple[0] for match_tuple in intitial_matches_to_scrape]))\n",
    "    print(f\"\\nüèì Matches to scrape: {intitial_matches_to_scrape_count} across {intitial_events_to_scrape_count} events üèì\")\n",
    "    \n",
    "    # Get the initial list of tasks / matches to be scraped\n",
    "    \n",
    "    matches_to_scrape = intitial_matches_to_scrape \n",
    "    # List to hold ALL successful ¬†and failed results from all attempts\n",
    "    all_successful_data = []    \n",
    "    failures = [] \n",
    "    # set retries count to 0 \n",
    "    retries = 0\n",
    "    # Start total timer\n",
    "    global_start_time = time.time() \n",
    "\n",
    "\n",
    "    ############################ START OF MAIN ASYNC LOOP  #####################################\n",
    "\n",
    "    # while loop - keep trying if there are still matches to get or until max retries count is reached\n",
    "    while (retries < MAX_RETRIES) and bool(matches_to_scrape):\n",
    "        retries += 1 # Increment attempt counter (Attempt 1, 2, ...)\n",
    "        \n",
    "        # print intial scraping started\n",
    "        if retries == 1:\n",
    "            print(f\"--- üöÄ Starting initial scrape for {len(matches_to_scrape)} matches... ---\")\n",
    "        else:\n",
    "            # log if a retry has started\n",
    "            print(f\"\\n--- üîÑ Starting Retry {retries-1}/{MAX_RETRIES-1} for {len(matches_to_scrape)} remaining matches... ---\")\n",
    "        \n",
    "        # start scraping and log retry start time (useful if one retry goes awry)\n",
    "        attempt_start_time = time.time()\n",
    "        \n",
    "        # run the async fetching \n",
    "        successes, failures = await (main_scraper(tuples_to_scrape=matches_to_scrape))\n",
    "        \n",
    "        \n",
    "        # process results as they finish\n",
    "        \n",
    "        # Add new successes to the successes list \n",
    "        all_successful_data.extend(successes)\n",
    "        \n",
    "        # after each retry - update new matches to scrape list for next loop\n",
    "\n",
    "        matches_to_scrape = [(event_id, match_code)for event_id, match_code, error_msg in failures]\n",
    "        \n",
    "        # log results of the retry attempt dependent on success / fails\n",
    "        attempt_duration = time.time() - attempt_start_time\n",
    "        if successes: \n",
    "            print(f\"--- Attempt {retries} finished in {attempt_duration:.2f}s. Got {len(successes)} new results. ---\")\n",
    "        if failures: \n",
    "            print(f\"--- {len(failures)} tasks failed and will be retried. ---\")\n",
    "\n",
    "    ################################### END OF MAIN ASYNC LOOP #####################################\n",
    "\n",
    "    # Final Printing summary (OUTSIDE OF MAIN LOOP) \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    \n",
    "    # if no matches left to scrape, then ¬†log success :) \n",
    "    if not matches_to_scrape: \n",
    "        print(\"--- ‚úÖüèìüü¢ Scraping Complete. All tasks finished successfully. üü¢üèì‚úÖ---\")\n",
    "    else:\n",
    "        # 'failures' holds the leftover failures ¬†from the LAST attempt\n",
    "        print(f\"--- ‚ö†Ô∏è Scraping Complete. {len(failures)} tasks permanently failed after {MAX_RETRIES} attempts. ---\")\n",
    "        \n",
    "    print(f\"Total successful matches collected: {len(all_successful_data)}\")\n",
    "    total_duration = time.time() - global_start_time\n",
    "    print(f\"Total Wall Clock Time: {total_duration:.2f} seconds.\")\n",
    "\n",
    "    # group all data \n",
    "    grouped_details = group_match_details(all_successful_data) \n",
    "    # log events saved (useful if errors occur)\n",
    "    events_saved_count = 0 \n",
    "\n",
    "    print(\"--- üíæ Saving data to disk (Read-Modify-Write) ---\")\n",
    "    for event_id, new_matches_list in grouped_details.items():\n",
    "        output_filename = os.path.join(RAW_MATCH_DETAILS_DIR, f\"{event_id}_match_details.json\")\n",
    "        \n",
    "        existing_matches_list = []\n",
    "        \n",
    "        # Read file and check if the file exists and read its contents.\n",
    "        if os.path.exists(output_filename) and os.path.getsize(output_filename) > 0:\n",
    "            try:\n",
    "                with open(output_filename, 'r') as f:\n",
    "                    existing_matches_list = json.load(f)\n",
    "                if not isinstance(existing_matches_list, list):\n",
    "                    print(f\"‚ö†Ô∏è Warning: Data in {output_filename} was not a list. Overwriting.\")\n",
    "                    existing_matches_list = []\n",
    "            except (json.JSONDecodeError, Exception) as e:\n",
    "                print(f\"‚ö†Ô∏è Warning: Could not read/decode {output_filename} ({e}). Overwriting.\")\n",
    "                existing_matches_list = []\n",
    "\n",
    "        # log all unique matches processed \n",
    "        existing_doc_codes = {match.get('documentCode') for match in existing_matches_list if isinstance(match, dict)}\n",
    "        added_count = 0\n",
    "        \n",
    "        for match in new_matches_list:\n",
    "            if isinstance(match, dict) and match.get('documentCode') not in existing_doc_codes:\n",
    "                existing_matches_list.append(match)\n",
    "                added_count += 1\n",
    "        \n",
    "       \n",
    "        try:\n",
    "            with open(output_filename, 'w') as f:\n",
    "                json.dump(existing_matches_list, f, indent=4)\n",
    "            \n",
    "            events_saved_count += 1\n",
    "            print(f\"‚úÖ Updated Event {event_id}: Added {added_count} new matches (Total: {len(existing_matches_list)}).\", end='\\r')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR saving Event {event_id}: {type(e).__name__}\".ljust(80))\n",
    "    \n",
    "\n",
    "    print(f\"\\n--- ‚úÖ All successful events written to disk. ({events_saved_count} files) ---\")\n",
    "\n",
    "    # if failures exists, print and save for future reference\n",
    "    if failures: \n",
    "        failures_df = pd.DataFrame(failures, columns=[\"eventId\", \"matchCode\",\"failureReason\"])\n",
    "        try:\n",
    "            # Save the failures to a csv for reference\n",
    "            failures_df.to_csv(FAILURE_LOG_PATH, index=False)\n",
    "            \n",
    "            print(f\"--- ‚úÖ Failure log saved for {len(failures_df)} tasks to {FAILURE_LOG_PATH} ---\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"--- ‚ùå FAILED to save failure log: {e} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c80a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "table_tennis_stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
