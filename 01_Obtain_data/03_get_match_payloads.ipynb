{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5f52b33-4dd4-4609-9c6a-6ef248415876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sys\n",
    "import concurrent.futures \n",
    "from typing import Union, Dict, List, Tuple, Any\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6c1058-028a-4382-9e9d-ffbdaa5f5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script fetches the raw JSON containing all \"match payloads\" containing match metadata\n",
    "    for a given WTT event ID using a GET request.\n",
    "    \n",
    "    Match-codes contained in payload are reequired for subsequent API call to get full match details.\n",
    "    \n",
    "    Reverse engineered from WTT events pages such as:\n",
    "    https://www.worldtabletennis.com/eventInfo?eventId=3085&selectedTab=Matches\n",
    "\n",
    "    Events_file is a csv containing the events list of events to be scraped based on their unique event ID.\n",
    "\n",
    "    A csv file is made for each event containing all match payloads for that event.\n",
    "\n",
    "    Threading has been implemented to speed up the proccess.\n",
    "    \n",
    "\"\"\"\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# Specifying the csv containing all the events from\n",
    "EVENTS_FILE = \"../Data/Processed/Events/shortlist_events.csv\"\n",
    "\n",
    "# A csv for each event containing its match payloads will be saved to this directory/\n",
    "OUTPUT_DIR = \"../Data/Raw/Match_payloads\"\n",
    "\n",
    "# Values used to generate random pause duration in seconds for API politeness\n",
    "MIN_PAUSE = 0.1 \n",
    "MAX_PAUSE = 0.2 \n",
    "\n",
    "# Number of threads for the IO processing.\n",
    "# Based on reading - 20 is a good starting number:\n",
    "MAX_WORKERS = 20 \n",
    "\n",
    "# Max retries for fdailed requests\n",
    "MAX_RETRIES = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3776bfc0-dfdb-47e6-aa40-da7173780d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_save_payloads(event_id: Union[int, str], output_dir: str, min_pause: float, max_pause: float) -> Tuple[int, bool, int, str]:\n",
    "    \"\"\"\n",
    "    For one event: fetches match payloads, saves to CSV, and handles errors and reporting.\n",
    "    For use with threadpool workers\n",
    "    Returns: (event_id, status_bool, match_count, status_message)\n",
    "    \"\"\"\n",
    "\n",
    "    # Define API endpoint URL and necessary params + headers.\n",
    "    url = \"https://liveeventsapi.worldtabletennis.com/api/cms/GetOfficialResult\"\n",
    "    params = {'EventId': str(event_id), \"DocumentCode\": \"TTE\"}\n",
    "    headers = {\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Referer': 'https://www.worldtabletennis.com/',\n",
    "        'User-Agent': 'Mozilla/5.0 (Linux; Android 11.0; Surface Duo) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Mobile Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # initialise variables for match_count and the status message for logging.\n",
    "    match_count = 0\n",
    "    status_msg = \"\"\n",
    "\n",
    "    # Try the api call and get response as json. Timeout set to keep thread running if API TimeoutError occurs\n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers, timeout=20)\n",
    "        \n",
    "        # Raise exception for bad status codes (4xx request errors or 5xx server errors)\n",
    "        response.raise_for_status()\n",
    "        raw_payloads = response.json() \n",
    "\n",
    "        # Check if response is a list as expected.\n",
    "        if not isinstance(raw_payloads, list):\n",
    "            # return content for logging, type.__name__ isolates typename as string  \n",
    "            status_msg = f\"JSON was not a list ({type(raw_payloads).__name__})\"\n",
    "            # return content for logginga\n",
    "            return event_id, False, 0, status_msg\n",
    "\n",
    "        # convert match payloads to df and get length / number of matches\n",
    "        payloads_df = pd.DataFrame(raw_payloads)\n",
    "        match_count = len(payloads_df)\n",
    "\n",
    "        # Create filename and save df to csv (even if content is blank)\n",
    "        filename = os.path.join(OUTPUT_DIR,f\"{event_id}_match_payloads.csv\")\n",
    "        payloads_df.to_csv(filename, index=False)\n",
    "\n",
    "        sleep_duration = random.uniform(min_pause, max_pause)\n",
    "        time.sleep(sleep_duration)\n",
    "\n",
    "        return event_id, True, match_count, f\"Found {match_count} matches for event:{event_id}. Pausing for {sleep_duration:.1f}s.\"\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        status_msg = f\"HTTP Error: {e.response.status_code}\"\n",
    "    except Exception as e:\n",
    "        status_msg = f\"Error: {type(e).__name__}\"\n",
    "\n",
    "    return event_id, False, 0, status_msg \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2f3ec43-49ff-425b-a5ea-0f9489c64c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_events_to_scrape(shortlist_df: pd.DataFrame, output_dir: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Checks the event_shortlist and output directory to return event_ids\n",
    "    that need to be scraped. An event needs scraping if it is not completed OR\n",
    "    if its payload file does not exist.\n",
    "\n",
    "    Args:\n",
    "        shortlist_df (pd.DataFrame): Shortlist of events to be scraped (must contain 'eventId' and 'Completed').\n",
    "        output_dir (str): The directory where existing match payloads are saved.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A series of event_ids that need to be scraped.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use the input DF length for initial count\n",
    "    events_total_initial = len(shortlist_df)\n",
    "\n",
    "    # --- NEW LOGIC: Initialize list for IDs TO SCRAPE ---\n",
    "    ids_to_scrape: List[int] = []\n",
    "    events_checked_count = 0\n",
    "\n",
    "    print(f\"\\n--- 🟠 Starting Check on {events_total_initial} events to determine scrape list... 🟠 ---\")\n",
    "\n",
    "    # Ensure required columns have correct types for comparison\n",
    "    try:\n",
    "        ongoing_events_df = shortlist_df[shortlist_df['EventStatus'] == 'Ongoing']\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"--- ❌ ERROR: Missing required column '{e}' in shortlist DataFrame. Cannot proceed. ---\")\n",
    "        return pd.Series([], dtype=int) # Return empty Series\n",
    "\n",
    "    # loop through the shortlist dataframe\n",
    "    for index, event_row in shortlist_df.iterrows():\n",
    "        events_checked_count += 1\n",
    "        event_id = event_row['eventId']\n",
    "        is_ongoing = event_row['EventStatus']==\"Ongoing\"\n",
    "\n",
    "        # Construct the expected payload filename\n",
    "        payload_file = os.path.join(output_dir, f\"{event_id}_match_payloads.csv\")\n",
    "\n",
    "      \n",
    "        # Scrape if EITHER:\n",
    "        # tje event is NOT completed (i.e mathches are still being played)\n",
    "        # or the payload file does NOT exist (not yet been scraped)\n",
    "        if is_ongoing or not os.path.exists(payload_file):\n",
    "            ids_to_scrape.append(event_id)\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "    # Filter the original DataFrame on the IDs identified for scraping\n",
    "    \n",
    "    events_to_scrape_df = shortlist_df[shortlist_df['eventId'].isin(ids_to_scrape)].copy()\n",
    "\n",
    "    events_to_scrape_count = len(events_to_scrape_df)\n",
    "    already_obtained_and_completed_count = events_total_initial - events_to_scrape_count # Calculate skipped count\n",
    "\n",
    "    print(f\"\\n--- CHECK COMPLETE: {events_to_scrape_count}/{events_total_initial} events identified for scraping. ---\")\n",
    "    print(f\"✅ Total Events: {events_total_initial} | Already Obtained & Completed: {already_obtained_and_completed_count} | To Scrape: {events_to_scrape_count}\")\n",
    "\n",
    "    # Return the clean Series of IDs that need scraping\n",
    "    return events_to_scrape_df[\"eventId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5db6801e-6bb6-4163-902e-e3f50d3ec80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---🚀 Starting Obtaining Match Payloads 🚀---\n",
      "\n",
      "--- 🟠 Starting Check on 302 events to determine scrape list... 🟠 ---\n",
      "\n",
      "--- CHECK COMPLETE: 1/302 events identified for scraping. ---\n",
      "✅ Total Events: 302 | Already Obtained & Completed: 301 | To Scrape: 1\n",
      "\n",
      "---🚀 Starting Initial Concurrent Scraping for 1 Events 🚀---\n",
      "--- Using 20 threads. API pause: 0.1s - 0.2s ---\n",
      "\n",
      "--- Initial scraping phase complete. 1/1 succeeded initially. ---\n",
      "\n",
      "==================================================\n",
      "✅ Finished! Match payloads obtained for 1/1 requested events (including retries).\n",
      "18  matches found for new / ongoing events\n",
      "Total run time = 0 m and 1 s.\n",
      "---🟢 Scraping finished. 🟢---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    start_time = time.time() # Start timing the entire run\n",
    "    print(\"---🚀 Starting Obtaining Match Payloads 🚀---\")\n",
    "\n",
    "    #  Perform Skip Check on Shortlist DF and\n",
    "    try:\n",
    "        shortlist_df = pd.read_csv(EVENTS_FILE)              \n",
    "    except FileNotFoundError:\n",
    "        print(f\"--- ❌ ERROR: Shortlist file not found at {EVENTS_FILE}. ---\"); sys.exit(1)\n",
    "  \n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    events_to_scrape_ids = filter_events_to_scrape(shortlist_df, OUTPUT_DIR)\n",
    "    event_ids_to_process = events_to_scrape_ids.tolist()\n",
    "    events_to_scrape_count = len(event_ids_to_process)\n",
    "\n",
    "    if events_to_scrape_count == 0:\n",
    "        print(\"\\n--- ✅ PROCESS COMPLETE: No events remaining to scrape. ---\"); sys.exit(0)\n",
    "\n",
    "    # --- Counters & Setup ---\n",
    "    processed_count = 0\n",
    "    successful_count = 0\n",
    "    failed_event_ids: List[int] = [] # Initial list for failures\n",
    "    start_time_loop = time.time()\n",
    "    new_payoads_count = 0\n",
    "\n",
    "    print(f\"\\n---🚀 Starting Initial Concurrent Scraping for {events_to_scrape_count} Events 🚀---\")\n",
    "    \n",
    "    print(f\"--- Using {MAX_WORKERS} threads. API pause: {MIN_PAUSE:.1f}s - {MAX_PAUSE:.1f}s ---\")\n",
    "\n",
    "    # --- Initial Parallel Execution ---\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {\n",
    "            # Ensure MIN_PAUSE_S and MAX_PAUSE_S constants are passed\n",
    "            executor.submit(fetch_and_save_payloads, event_id, OUTPUT_DIR, MIN_PAUSE, MAX_PAUSE): event_id\n",
    "            for event_id in event_ids_to_process\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            processed_count += 1\n",
    "            event_id = futures[future]\n",
    "            try:\n",
    "                result_id, status, match_count, status_msg = future.result()\n",
    "                if status:\n",
    "                    successful_count += 1\n",
    "                    new_payoads_count += match_count\n",
    "                    # --- SUCCESS: No print ---\n",
    "                else:\n",
    "                    failed_event_ids.append(event_id)\n",
    "                    # --- FAILURE: Print error message ---\n",
    "                    print(f\"Event {event_id}: ({processed_count}/{events_to_scrape_count})  ❌ Failed: {status_msg}\")\n",
    "\n",
    "                # --- Checkpoint Log (Every 10 events) ---\n",
    "                if processed_count % 10 == 0:\n",
    "                    elapsed_time = time.time() - start_time_loop\n",
    "                    minutes = int(elapsed_time // 60); seconds = int(elapsed_time % 60)\n",
    "                    # Print checkpoint summary on a new line\n",
    "                    print(f\"\\n---  {processed_count}/{events_to_scrape_count} processed. Success Rate: {successful_count/processed_count:.1%}. Elapsed: {minutes}m {seconds}s ---\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Print fatal errors during result retrieval\n",
    "                print(f\"\\n--- ❌ FATAL ERROR processing result for Event {event_id}: {type(e).__name__} ---\")\n",
    "                failed_event_ids.append(event_id)\n",
    "\n",
    "    print(f\"\\n--- Initial scraping phase complete. {successful_count}/{events_to_scrape_count} succeeded initially. ---\")\n",
    "\n",
    "\n",
    "    # --- RETRY LOGIC BLOCK ---\n",
    "    retry_attempt = 0\n",
    "    while failed_event_ids and retry_attempt < MAX_RETRIES:\n",
    "        retry_attempt += 1\n",
    "        ids_to_retry = list(failed_event_ids)\n",
    "        failed_event_ids.clear()\n",
    "        current_retry_successful_count = 0\n",
    "        current_retry_processed_count = 0\n",
    "\n",
    "        # --- Print Retry Start ---\n",
    "        print(f\"\\n--- Starting retry number {retry_attempt}/{MAX_RETRIES} for {len(ids_to_retry)} failed events ---\")\n",
    "        time.sleep(2) # Small pause before retry batch\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor_retry:\n",
    "            futures_retry = {\n",
    "                \n",
    "                executor_retry.submit(fetch_and_save_payloads, event_id, OUTPUT_DIR, MIN_PAUSE, MAX_PAUSE): event_id\n",
    "                for event_id in ids_to_retry\n",
    "            }\n",
    "\n",
    "            for future_retry in concurrent.futures.as_completed(futures_retry):\n",
    "                current_retry_processed_count += 1\n",
    "                event_id_retry = futures_retry[future_retry]\n",
    "                try:\n",
    "                    result_id_retry, status_retry, match_count_retry, status_msg_retry = future_retry.result()\n",
    "                    if status_retry:\n",
    "                        current_retry_successful_count += 1\n",
    "                        new_payoads_count += match_count\n",
    "                        # --- SUCCESS (Retry): No print ---\n",
    "                    else:\n",
    "                        failed_event_ids.append(event_id_retry)\n",
    "                        # --- FAILURE (Retry): Print error message ---\n",
    "                        print(f\"Retry {retry_attempt}: Event {event_id_retry} ({current_retry_processed_count}/{len(ids_to_retry)}) ❌ Failed again: {status_msg_retry}\")\n",
    "\n",
    "                    # --- Checkpoint Log (Every 10 events within retry batch) ---\n",
    "                    # Note: Using current_retry_processed_count here\n",
    "                    if current_retry_processed_count % 10 == 0:\n",
    "                        elapsed_time = time.time() - start_time_loop # Still measure from loop start\n",
    "                        minutes = int(elapsed_time // 60); seconds = int(elapsed_time % 60)\n",
    "                        # Calculate success rate for *this retry batch*\n",
    "                        batch_success_rate = current_retry_successful_count / current_retry_processed_count if current_retry_processed_count > 0 else 0\n",
    "                        print(f\"\\n--- RETRY CHECKPOINT: {current_retry_processed_count}/{len(ids_to_retry)} processed in attempt {retry_attempt}. Batch Success: {batch_success_rate:.1%}. Total Elapsed: {minutes}m {seconds}s ---\")\n",
    "\n",
    "                except Exception as e_retry:\n",
    "                     print(f\"\\n--- ❌ FATAL ERROR during retry for Event {event_id_retry}: {type(e_retry).__name__} ---\")\n",
    "                     failed_event_ids.append(event_id_retry)\n",
    "\n",
    "        successful_count += current_retry_successful_count # Add successful retries to total\n",
    "        print(f\"\\n--- Retry Attempt {retry_attempt} complete. {current_retry_successful_count}/{len(ids_to_retry)} succeeded this attempt. ---\")\n",
    "        if failed_event_ids:\n",
    "            print(f\"--- {len(failed_event_ids)} events still failing after {retry_attempt} retries. ---\")\n",
    "\n",
    "    # --- End of Retry Logic ---\n",
    "\n",
    "\n",
    "    # --- Final Summary ---\n",
    "    total_run_time = time.time() - start_time\n",
    "    total_minutes = int(total_run_time // 60)\n",
    "    total_seconds = int(total_run_time % 60)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"✅ Finished! Match payloads obtained for {successful_count}/{events_to_scrape_count} requested events (including retries).\")\n",
    "    print(f\"{new_payoads_count}  matches found for new / ongoing events\")\n",
    "    if failed_event_ids:\n",
    "        print(f\"⚠️ Permanently Failed Event IDs ({len(failed_event_ids)}): {failed_event_ids}\")\n",
    "    print(f\"Total run time = {total_minutes} m and {total_seconds} s.\")\n",
    "    print(\"---🟢 Scraping finished. 🟢---\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e16d4593",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shortlist_df_object' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--- ❌ ERROR: Shortlist file not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEVENTS_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. ---\u001b[39m\u001b[33m\"\u001b[39m); sys.exit(\u001b[32m1\u001b[39m)\n\u001b[32m     13\u001b[39m os.makedirs(OUTPUT_DIR, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m events_to_scrape_ids = filter_events_to_scrape(\u001b[43mshortlist_df_object\u001b[49m, OUTPUT_DIR)\n\u001b[32m     15\u001b[39m event_ids_to_process = events_to_scrape_ids.tolist()\n\u001b[32m     16\u001b[39m events_to_scrape_count = \u001b[38;5;28mlen\u001b[39m(event_ids_to_process)\n",
      "\u001b[31mNameError\u001b[39m: name 'shortlist_df_object' is not defined"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd0420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "table_tennis_stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
