{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fa0044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee7fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_MATCH_DETAILS_DIR = \"../Data/Raw/Match_details\"\n",
    "\n",
    "DNF_KEYWORDS = ['WO', 'INJ', 'RET', 'DSQ', 'DNS']\n",
    "DNF_PATTERN = '|'.join(DNF_KEYWORDS)\n",
    "DNF_PATTERN_CAPTURE = r'(' + r'|'.join(DNF_KEYWORDS) + r')'\n",
    "\n",
    "\n",
    "# drop some columns that are not currently of interest for the project (e.g table number and venue))\n",
    "# result status is \"offical\" for all entries - no neeed to keep\n",
    "DROP_COLUMNS_START = [\n",
    "    \"resultStatus\",        \n",
    "    \"playByPlaySequenceNumber\"]\n",
    "\n",
    "EVENTS_FILE = \"../Data/Processed/Events/shortlist_events.csv\"\n",
    "\n",
    "CLEANED_MATCHES_DIR = \"../Data/Processed/Matches\"\n",
    "\n",
    "\n",
    "JUNK_PATTERN_CAPTURE = r'([^\\d,-]+)'\n",
    "JUNK_PATTERN_CLEAN = r'[^\\d,-]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all match files in the RAW_MATCH_DETAILS_DIR\n",
    "# Create an all_matches_df to be filtered down \n",
    "# Many of these matches are in fact match ups between teams rather than players - need to filter out.\n",
    "# some of the singles matches are from teams matches and events - can keep these.\n",
    "\n",
    "\n",
    "\n",
    "# parse all match details inside the json files.\n",
    "all_match_details_files = glob.glob(os.path.join(RAW_MATCH_DETAILS_DIR, \"*.json\"))\n",
    "all_matches = []\n",
    "for file in all_match_details_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        matches_data = json.load(f)\n",
    "\n",
    "    all_matches.extend(matches_data)\n",
    "\n",
    "# create the df \n",
    "\n",
    "all_matches_df = pd.DataFrame(all_matches)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep here inside this cell to prevent rerunning file parsing and extraction every time.\n",
    "all_matches_df = pd.DataFrame(all_matches)\n",
    "\n",
    "# drop some columns that are not currently of interest for the project (e.g table number and venue etc )\n",
    "\n",
    "\n",
    "\n",
    "# initialise the cleaned matches df and drop empty columns and na rows\n",
    "print(f\"cleaned_matches_df before  dropna and dropping irrelevant columns: {len(all_matches_df)} with {len(all_matches_df.columns)} cols\")\n",
    "cleaned_matches_df = all_matches_df\n",
    "cleaned_matches_df.dropna(axis=0, how='all', inplace=True)\n",
    "cleaned_matches_df.dropna(axis=1, how='all', inplace=True)\n",
    "cleaned_matches_df.drop(columns=DROP_COLUMNS_START, inplace=True)\n",
    "print(f\"cleaned_matches_df after dropna and dropping irrelevant columns: {len(cleaned_matches_df)} with {len(cleaned_matches_df.columns)} cols\")\n",
    "\n",
    "\n",
    "dnf_from_overallScores = cleaned_matches_df[\"overallScores\"].str.extract(JUNK_PATTERN_CAPTURE, expand=False).str.strip()\n",
    "dnf_from_resultoverallScores = cleaned_matches_df[\"resultOverallScores\"].str.extract(JUNK_PATTERN_CAPTURE, expand=False).str.strip()\n",
    "dnf_from_gameScores = cleaned_matches_df[\"gameScores\"].str.extract(JUNK_PATTERN_CAPTURE, expand=False).str.strip()\n",
    "dnf_from_resultsGameScores = cleaned_matches_df[\"resultsGameScores\"].str.extract(JUNK_PATTERN_CAPTURE, expand=False).str.strip()\n",
    "\n",
    "cleaned_matches_df[\"overallScores\"] = cleaned_matches_df[\"overallScores\"].str.replace(JUNK_PATTERN_CLEAN, '', regex=True)\n",
    "cleaned_matches_df[\"resultOverallScores\"] = cleaned_matches_df[\"resultOverallScores\"].str.replace(JUNK_PATTERN_CLEAN, '', regex=True)\n",
    "cleaned_matches_df[\"gameScores\"] = cleaned_matches_df[\"gameScores\"].str.replace(JUNK_PATTERN_CLEAN, '', regex=True)\n",
    "cleaned_matches_df[\"resultsGameScores\"] = cleaned_matches_df[\"resultsGameScores\"].str.replace(JUNK_PATTERN_CLEAN, '', regex=True)\n",
    "\n",
    "\n",
    "\n",
    "cleaned_matches_df[\"dnf\"] = dnf_from_overallScores\n",
    "cleaned_matches_df[\"dnf\"] = cleaned_matches_df[\"dnf\"].fillna(dnf_from_resultoverallScores)\n",
    "cleaned_matches_df[\"dnf\"] = cleaned_matches_df[\"dnf\"].fillna(False)\n",
    "\n",
    "SCORE_RENAME_DICT = {\n",
    "    # rename columns to A and B for better clarity\n",
    "    \"gameScores\": \"A_rawGameScores\",\n",
    "    \"resultsGameScores\": \"B_rawGameScores\",\n",
    "    \"overallScores\": \"A_rawOverallScore\",\n",
    "    \"resultOverallScores\": \"B_rawOverallScore\"\n",
    "}\n",
    "\n",
    "cleaned_matches_df.rename(columns=SCORE_RENAME_DICT, inplace=True)\n",
    "\n",
    "print(\"âœ… 'dnf' column updated by sequentially filling missing values from 'overallScores' THEN 'resultOverallScores'.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05fb043",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"cleaned_df before dropping names with teams_parent_data: {len(cleaned_matches_df)}\")\n",
    "team_parent_filter = cleaned_matches_df[\"teamParentData\"].notna()\n",
    "team_parent_df  = cleaned_matches_df[team_parent_filter].copy()\n",
    "cleaned_matches_df = cleaned_matches_df[~team_parent_filter ].copy()\n",
    "\n",
    "# also drop team summaries\n",
    "for col in cleaned_matches_df.columns:\n",
    "    if \"team\" in col.lower():\n",
    "        cleaned_matches_df.drop(columns=[col], inplace=True)\n",
    "\n",
    "\n",
    "print(f\"cleaned_df after dropping names with teams_parent_data: {len(cleaned_matches_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea382b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"cleaned_df before dropping para and age limit matches: {len(cleaned_matches_df)}\")\n",
    "age_limit_mask = cleaned_matches_df['subEventName'].str.contains(r\"U\\d{2}\", case=False, na=False)\n",
    "para_class_mask = cleaned_matches_df['subEventName'].str.contains(\"class\", case=False, na=False)\n",
    "age_para_filter = age_limit_mask | para_class_mask\n",
    "cleaned_matches_df = cleaned_matches_df[~age_para_filter].copy()\n",
    "\n",
    "\n",
    "print(f\"cleaned_df after dropping para and age limit matches: {len(cleaned_matches_df)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30cca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before filtering - extact key information from the 'competitors' column\n",
    "# player name column can contain team names.\n",
    "# competitors column keeps track of either the 1 player for a singles listing\n",
    "# or the multiple players for a team listing \n",
    "# some doubles matches may be leftover from payload filtering - need to filter out.\n",
    "# some doubles matches may be here inside team events - need to filter out.\n",
    "\n",
    "def extract_competitor_details(competitor_list):\n",
    "    \"\"\"\n",
    "    Extracts only the top-level competitor details (Name, ID, ORG) \n",
    "    for Home (H) and Away (A) competitors, ignoring the nested 'players' dict.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the output dictionary  \n",
    "    data = {}\n",
    "    \n",
    "    # check for empty data incase \n",
    "    if not isinstance(competitor_list, list) or len(competitor_list) < 2:\n",
    "        return pd.Series(data)\n",
    "\n",
    "    try:\n",
    "        # use prefix pattern to determine home/away and build the keys for the output dict.\n",
    "        for comp in competitor_list:\n",
    "            comp_type = comp.get('competitorType')\n",
    "            \n",
    "            if comp_type == 'H':\n",
    "                prefix = 'home'\n",
    "            elif comp_type == 'A':\n",
    "                prefix = 'away'\n",
    "            else:\n",
    "                continue                \n",
    "           \n",
    "            \n",
    "            # get competitor id \n",
    "            data[f'{prefix}CompetitorId'] = comp.get('competitiorId', pd.NA)\n",
    "                                \n",
    "            # get competitor country code \n",
    "            data[f'{prefix}CompetitorOrg'] = comp.get('competitiorOrg', pd.NA)\n",
    "\n",
    "            data[f'{prefix}Player'] = [player.get('playerName', pd.NA) for player in comp.get('players', pd.NA)]\n",
    "\n",
    "            data[f'{prefix}NestedGameScores'] = comp.get('scores', pd.NA)\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e} | Data: {competitor_list}\")\n",
    "        pass\n",
    "\n",
    "    return pd.Series(data)\n",
    "\n",
    "#\n",
    "print(\"--- ðŸš€ Getting competitor details ðŸš€ ---\")\n",
    "\n",
    "# apply the function to the competitors column from the main df\n",
    "competitor_details_df = cleaned_matches_df['competitiors'].apply(extract_competitor_details)\n",
    "\n",
    "\n",
    "\n",
    "cleaned_matches_df = pd.concat([cleaned_matches_df, competitor_details_df], axis=1)\n",
    "cleaned_matches_df.drop(columns=[\"competitiors\"],inplace=True, errors='ignore')\n",
    "cleaned_matches_df[\"homePlayer\"] = cleaned_matches_df[\"homePlayer\"].str[0]\n",
    "cleaned_matches_df[\"awayPlayer\"] = cleaned_matches_df[\"awayPlayer\"].str[0]\n",
    "\n",
    "\n",
    "\n",
    "print(\"âœ… Competitor details extracted and added to cleaned_matches_df and competitiors column dropped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ed928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All teams matches should be removed by now \n",
    "\n",
    "def extract_format(config):\n",
    "    \"\"\"\n",
    "    Attempts to extract the best of format from the 'matchConfig' column\n",
    "    \"\"\"\n",
    "    data = {\"bestOf\": pd.NA, \"ttrReview\": pd.NA}\n",
    "    if not isinstance(config,dict):\n",
    "        return pd.Series(data)\n",
    "    try:\n",
    "        data['bestOf'] = config.get('bestOfXGames')\n",
    "        data['ttrReview'] = config.get('tTRReview')\n",
    "        return pd.Series(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e} | Data: {config}\")\n",
    "        pass\n",
    "    return pd.Series(data)\n",
    "#\n",
    "print(\"--- ðŸš€ Getting Match Config deatils ðŸš€ ---\")\n",
    "\n",
    "# apply the function to the competitors column from the main df\n",
    "match_config_df = cleaned_matches_df['matchConfig'].apply(extract_format)\n",
    "\n",
    "\n",
    "cleaned_matches_df = pd.concat([cleaned_matches_df, match_config_df], axis=1)\n",
    "cleaned_matches_df.drop(columns=[\"matchConfig\"],inplace=True, errors='ignore')\n",
    "\n",
    "print(\"âœ… Match config extracted and added to cleaned_matches_df,matchConfig column dropped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serverNext = NEXT SERVER AFTER THE MATCH POINT WAS DONE\n",
    "# (even if point would not be played as match was over)\n",
    "\n",
    "def extract_next_server(action):\n",
    "    \"\"\"\n",
    "    Attempts to extract the best of format from the 'matchConfig' column\n",
    "    \"\"\"\n",
    "    data = {\"serverNext\":pd.NA,\n",
    "            \"actionType\": pd.NA\n",
    "}\n",
    "    if not isinstance(action,dict):\n",
    "        return pd.Series(action)\n",
    "    try:\n",
    "        \n",
    "        data[\"serverNext\"] = action.get(\"serverNext\")   \n",
    "        data[\"actionType\"] = action.get(\"actionType\")\n",
    "        return pd.Series(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e} | Data: {action}\")\n",
    "        pass\n",
    "    return pd.Series(data)\n",
    "#\n",
    "print(\"--- ðŸš€ Getting Match Config deatils ðŸš€ ---\")\n",
    "\n",
    "# apply the function to the competitors column from the main df\n",
    "last_server_df = cleaned_matches_df['action'].apply(extract_next_server)\n",
    "\n",
    "\n",
    "cleaned_matches_df = pd.concat([cleaned_matches_df, last_server_df], axis=1)\n",
    "cleaned_matches_df.drop(columns=[\"action\"],inplace=True, errors='ignore')\n",
    "\n",
    "print(\"âœ… Match lastServer extracted and added to cleaned_matches_df,matchConfig column dropped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_times(matchTime):\n",
    "    \"\"\"\n",
    "    Attempts to extract the match date and duration from the matchDate column\n",
    "    \"\"\"\n",
    "    data = {\"duration (unreliable)\": pd.NA, \"startDateLocal\": pd.NA, \"startDateUTC\": pd.NA}\n",
    "    if not isinstance(matchTime, dict):\n",
    "        return pd.Series(matchTime)\n",
    "    try:\n",
    "        data['duration (unreliable)'] = matchTime.get('duration')\n",
    "        data['startDateLocal'] = matchTime.get('startDateLocal')\n",
    "        data['startDateUTC'] = matchTime.get('startDateUTC')\n",
    "        return pd.Series(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e} | Data: {matchTime}\")\n",
    "        pass\n",
    "    return pd.Series(data)\n",
    "\n",
    "times_df  = cleaned_matches_df['matchDateTime'].apply(extract_times)\n",
    "\n",
    "cleaned_matches_df = pd.concat([cleaned_matches_df, times_df], axis=1)\n",
    "\n",
    "cleaned_matches_df['startDateLocal'] = pd.to_datetime(\n",
    "    cleaned_matches_df['startDateLocal'], errors='coerce', utc=False\n",
    ")\n",
    "cleaned_matches_df['startDateUTC'] = pd.to_datetime(\n",
    "    cleaned_matches_df['startDateUTC'], errors='coerce', utc=True\n",
    ")\n",
    "cleaned_matches_df['startDateLocal'] = cleaned_matches_df['startDateLocal']\n",
    "cleaned_matches_df['startDateUTC'] = cleaned_matches_df['startDateUTC']\n",
    "\n",
    "cleaned_matches_df.drop(columns=[\"matchDateTime\"],inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfc5f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df=pd.read_csv(EVENTS_FILE)\n",
    "events_df[\"StartDateTime\"] = pd.to_datetime(events_df[\"StartDateTime\"], errors='coerce', utc=False)\n",
    "events_df[\"EndDateTime\"] = pd.to_datetime(events_df[\"EndDateTime\"], errors='coerce', utc=False)\n",
    "\n",
    "event_dates_df = events_df[[\"EventName\",\"eventId\", \"StartDateTime\"]]\n",
    "event_dates_df = event_dates_df.rename(columns = {\"StartDateTime\":\"EventStartDate\"})\n",
    "cleaned_matches_df[\"eventId\"] = cleaned_matches_df[\"eventId\"].astype(int)\n",
    "cleaned_matches_df = cleaned_matches_df.merge(\n",
    "    event_dates_df, \n",
    "    on='eventId', \n",
    "    how='left',\n",
    "    validate='m:1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_matches_df = cleaned_matches_df.sort_values(by = [\"EventStartDate\", \"startDateLocal\", \"matchStartTimeUTC\"])\n",
    "\n",
    "DATE_COLS = [\n",
    "    \"matchStartTimeUTC\", # Already exists, just needs re-conversion for safety\n",
    "    \"startDateLocal\",    # Needs conversion (was in nested dict)\n",
    "    \"startDateUTC\",      # Needs conversion (was in nested dict)\n",
    "    \"EventStartDate\"     # The merged date (likely a date object or string)\n",
    "]\n",
    "\n",
    "\n",
    "for col in DATE_COLS:\n",
    "    # Check if the column exists in the DataFrame before trying to convert\n",
    "    if col in cleaned_matches_df.columns:\n",
    "        \n",
    "        # 1. Convert to Datetime (errors='coerce' handles bad strings, turning them to NaT)\n",
    "        # 2. Assign the result back to the same column\n",
    "        cleaned_matches_df[col] = pd.to_datetime(\n",
    "            cleaned_matches_df[col], \n",
    "            errors='coerce',\n",
    "            utc=True # Ensure the resulting datetime object is timezone-aware (UTC)\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"âœ… All  date / timecolumns standardized to UTC datetime dtype.\")\n",
    "\n",
    "# Check the dtypes to confirm the conversion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc32c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry for muscat 2025 has a clearly erroneous date.\n",
    "# For now, manually remove this date and let it be handled by fillNa hierarchy\n",
    "target_event = all_matches_df[\"eventId\"] == \"3084\"\n",
    "target_match = all_matches_df[\"documentCode\"] == \"TTEWSINGLES-----------GP11000400----------\"\n",
    "target_mask = target_event & target_match\n",
    "cleaned_matches_df.loc[target_mask,\"matchStartTimeUTC\"] = pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60248e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_hierarchy = ['matchStartTimeUTC',\n",
    "                   'startDateUTC',\n",
    "                   'startDateLocal',                  \n",
    "                   'EventStartDate']\n",
    "\n",
    "cleaned_matches_df['matchDate'] = cleaned_matches_df[dates_hierarchy[0]]\n",
    "for col in dates_hierarchy[1:]:\n",
    "    # This only fills rows where 'matchDate_filled' is currently NA\n",
    "    cleaned_matches_df['matchDate'] = cleaned_matches_df['matchDate'].fillna(\n",
    "        cleaned_matches_df[col]\n",
    "    )\n",
    "columns_to_drop = ['matchStartTimeUTC',\n",
    "                   'startDateUTC',\n",
    "                   'startDateLocal']\n",
    "cleaned_matches_df = cleaned_matches_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "\n",
    "### Some matches start times are hours before their event start times - we can't fix that.\n",
    "## not gonna break anything -i can't confirm which one is correct for each match / event.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bb190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nested_scores(home_points_str, away_points_str):\n",
    "    \"\"\"\n",
    "    Calculates the game scores and overall match score from point total strings.\n",
    "    Returns: (calc_game_scores_str, calc_overall_scores_str)\n",
    "    \"\"\"\n",
    "    # dont consider missing data \n",
    "    if pd.isna(home_points_str) or pd.isna(away_points_str):\n",
    "        return pd.Series([pd.NA, pd.NA])\n",
    "    \n",
    "    # split strings into list \n",
    "    try:\n",
    "        # Filter out empty strings and convert to int\n",
    "        home_points = [int(p.strip()) for p in str(home_points_str).split(',') if p.strip()]\n",
    "        away_points = [int(p.strip()) for p in str(away_points_str).split(',') if p.strip()]\n",
    "    except ValueError:\n",
    "        return pd.Series([pd.NA, pd.NA])\n",
    "\n",
    "    # Ensure lists are the same length\n",
    "    min_length = min(len(home_points), len(away_points))\n",
    "    \n",
    "    home_games_won = 0\n",
    "    away_games_won = 0\n",
    "    game_scores = []\n",
    "\n",
    "    # zip home and away points - itererate over both up to length of shortest list \n",
    "    for h_pts, a_pts in zip(home_points[:min_length], away_points[:min_length]):\n",
    "        \n",
    "        # Skip 0-0 games\n",
    "        if h_pts == 0 and a_pts == 0:\n",
    "            continue\n",
    "            \n",
    "        # Create game score string \n",
    "        game_scores.append(f'{h_pts}-{a_pts}')\n",
    "        \n",
    "        # higher points tally as game wins\n",
    "        if h_pts > a_pts:\n",
    "            home_games_won += 1\n",
    "        elif a_pts > h_pts:\n",
    "            away_games_won += 1\n",
    "            \n",
    "    # join games scores intp output string\n",
    "    calc_game_scores_str = ','.join(game_scores)\n",
    "    calc_overall_scores_str = f'{home_games_won}-{away_games_won}'\n",
    "    \n",
    "    return pd.Series([calc_game_scores_str, calc_overall_scores_str])\n",
    "\n",
    "print(\"--- ðŸš€ Calculating Game and Overall Scores from nested Home and Away game scores ðŸš€ ---\")\n",
    "\n",
    "# use .apply and lambdafunction onto whole df \n",
    "new_score_cols = cleaned_matches_df.apply(\n",
    "    lambda row: calculate_nested_scores(row['homeNestedGameScores'], row['awayNestedGameScores']),\n",
    "    axis=1,\n",
    "    result_type='expand'\n",
    ")\n",
    "\n",
    "# Add names to the columns\n",
    "new_score_cols.columns = ['calcNestedGameScores', 'calcNestedOverallScores']\n",
    "\n",
    "# concat  new columns + original df \n",
    "cleaned_matches_df = pd.concat([cleaned_matches_df, new_score_cols], axis=1)\n",
    "# cleaned_matches_df.drop(columns=[\"homePlayerGameScores\",\"awayPlayerGameScores\"],inplace=True,errors=\"ignore\")\n",
    "\n",
    "print(\"âœ… Game scores and overall match scores calculated and added.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82087eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise scores by removing 0-0 games (at end of scores i.e unplayed sets)\n",
    "cleaned_matches_df[\"A_rawGameScores\"] = cleaned_matches_df[\"A_rawGameScores\"].str.replace(',0-0', '')\n",
    "cleaned_matches_df[\"B_rawGameScores\"] = cleaned_matches_df[\"B_rawGameScores\"].str.replace(',0-0', '')\n",
    "\n",
    "# clean all scores by removing entries that don't have non-0 digits\n",
    "\n",
    "\n",
    "\n",
    "def clean_zero_scores(df: pd.DataFrame, score_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replaces score strings that contain NO digits from 1-9\n",
    "    These entries are usually 0-0 fillers that should be removed\n",
    "    to ease later processing.\n",
    "    Args:\n",
    "        df: The DataFrame to clean.\n",
    "        score_column: The name of the column containing the game score strings.\n",
    "\n",
    "    Returns:\n",
    "        The DataFrame with the cleaned score column.\n",
    "    \"\"\"\n",
    "    \n",
    "    non_zero_score_mask = df[score_column].str.contains(r'[1-9]', regex=True, na=False)\n",
    "    \n",
    "    # Get the mask for rows that DO NOT contain non-zero digits\n",
    "    mask = ~non_zero_score_mask    \n",
    "   \n",
    "    # replace the masked values with pd.NA\n",
    "    df.loc[mask, score_column] = pd.NA\n",
    "\n",
    "    # log to check it has worked :)\n",
    "    \n",
    "    print(f\"âœ… Replaced {mask.sum()} zero score strings in '{score_column}' with pd.NA.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "scores_columns = [col for col in cleaned_matches_df.columns if \"score\" in col.lower()]\n",
    "for column in scores_columns:\n",
    "    cleaned_matches_df = clean_zero_scores(cleaned_matches_df, column)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Triangulate and reconcile incosistent overall scores ####\n",
    "\n",
    "\n",
    "print(\"--- ðŸš€ reconciling overall scores ðŸš€ ---\")\n",
    "\n",
    "\n",
    "# strip remaining scores of strings before reconciling\n",
    "cleaned_matches_df[\"calcNestedGameScores\"] = cleaned_matches_df[\"calcNestedGameScores\"].str.replace(JUNK_PATTERN_CLEAN, '', regex=True)\n",
    "cleaned_matches_df[\"homeNestedGameScores\"] = cleaned_matches_df[\"homeNestedGameScores\"].str.replace(JUNK_PATTERN_CLEAN, '', regex=True)\n",
    "cleaned_matches_df[\"awayNestedGameScores\"] = cleaned_matches_df[\"awayNestedGameScores\"].str.replace(JUNK_PATTERN_CLEAN, '', regex=True)\n",
    "\n",
    "cleaned_matches_df[\"A_rawOverallScore\"] = cleaned_matches_df[\"A_rawOverallScore\"].str.replace(JUNK_PATTERN_CLEAN, '', regex=True)\n",
    "cleaned_matches_df[\"B_rawOverallScore\"] = cleaned_matches_df[\"B_rawOverallScore\"].str.replace(JUNK_PATTERN_CLEAN, '', regex=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# normalise scores for string comparison, s1, s2, s3 = scores to be reconciled. \n",
    "s1 = cleaned_matches_df['calcNestedOverallScores'].astype(str).str.strip().replace({'nan': '', 'NaT': '', '<NA>': ''})\n",
    "s2 = cleaned_matches_df['B_rawOverallScore'].astype(str).str.strip().replace({'nan': '', 'NaT': '', '<NA>': ''})\n",
    "s3 = cleaned_matches_df['A_rawOverallScore'].astype(str).str.strip().replace({'nan': '', 'NaT': '', '<NA>': ''})\n",
    "\n",
    "# use a temporary score_df to do work on the series\n",
    "temp_scores_df = pd.DataFrame({'s1': s1, 's2': s2, 's3': s3}, index=cleaned_matches_df.index)\n",
    "\n",
    " \n",
    "def check_overall_score_consistency(row):\n",
    "    \"\"\"\n",
    "    Checks if all *present* scores in the row are identical.\n",
    "    returns true if all scores agree, otherwise false\n",
    "    \"\"\"\n",
    "\n",
    "    # temp score df only contains scores so just need to compare all rows\n",
    "    valid_scores = set([s for s in row if s != \"\"])\n",
    "    \n",
    "    # if set is length 1 or 0 - scores are consistent. \n",
    "    return len(valid_scores) == 1\n",
    "\n",
    "# apply the scores_check\n",
    "scoreConsistenct = temp_scores_df.apply(check_overall_score_consistency, axis=1)\n",
    "\n",
    "\n",
    "# Set dnf matches to be consistentScore = true, as these matches are a separate case.\n",
    "is_dnf = cleaned_matches_df['dnf'] != False \n",
    "\n",
    "# create column in the orginal to flag inconsistent scores (for future reference if needed)\n",
    "cleaned_matches_df['scoreConsistent'] = scoreConsistenct | is_dnf\n",
    "\n",
    "\n",
    "# compare all scores - if 2 or more agree - that value is used\n",
    "# otherwise scores are taken in order of the s1,s2,s3\n",
    "def get_democratic_overall_score(row):\n",
    "    valid_scores = [s for s in row if s!= \"\"]\n",
    "    if not valid_scores: return \"\"\n",
    "    mode_result = pd.Series(valid_scores).mode()\n",
    "    \n",
    "    if not mode_result.empty:\n",
    "        return mode_result.iloc[0] \n",
    "    else:\n",
    "        return row['s1'] # Fallback\n",
    "\n",
    "cleaned_matches_df['reconciledOverallScore'] = temp_scores_df.apply(get_democratic_overall_score, axis=1)\n",
    "# cleaned_matches_df.drop(columns=[\"resultOverallScores\",\"overallScores\", \"calcOverallScores\"],inplace=True, errors=\"ignore\")\n",
    "\n",
    "print(\"--- âœ”ï¸ Scores Triangulated / reconciliated âœ”ï¸ ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bf636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_matches_df.fillna(value=pd.NA,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3bd381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which scores are missing\n",
    "\n",
    "missing_A_gameScores = cleaned_matches_df[\"A_rawGameScores\"].isnull()\n",
    "missing_B_gameScores = cleaned_matches_df[\"B_rawGameScores\"].isnull() \n",
    "missing_calcNestedGameScores = cleaned_matches_df[\"calcNestedGameScores\"].isnull()\n",
    "not_dnf = cleaned_matches_df[\"dnf\"] == False\n",
    "\n",
    "missing_A_gameScores_df = cleaned_matches_df[missing_A_gameScores & not_dnf]\n",
    "missing_B_gameScores_df = cleaned_matches_df[missing_B_gameScores & not_dnf]\n",
    "missing_calcGameScores_df = cleaned_matches_df[missing_calcNestedGameScores & not_dnf]\n",
    "print(f\"Missing A_gameScores: {len(missing_A_gameScores_df)}\")\n",
    "print(f\"Missing B_gameScores: {len(missing_B_gameScores_df)}\")\n",
    "print(f\"Missing calcNestedGameScores: {len(missing_calcGameScores_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d78121",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_matches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2837c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_score_winner(row):\n",
    "\n",
    "    try:    \n",
    "        scores = row[\"reconciledOverallScore\"]\n",
    "        score_split = scores.split(\"-\")\n",
    "        home_score = int(score_split[0])\n",
    "        away_score = int(score_split[1])\n",
    "        \n",
    "        if home_score > away_score:\n",
    "            return \"home\"\n",
    "        elif away_score > home_score:\n",
    "            return \"away\"\n",
    "        else:\n",
    "            return \"tie\"\n",
    "    except:\n",
    "        return pd.NA\n",
    "        \n",
    "\n",
    "\n",
    "cleaned_matches_df[\"reconciledOverallScoreWinner\"] = cleaned_matches_df.apply(get_overall_score_winner, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325bab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Triangulate and reconcile incosistent overall scores ####\n",
    "\n",
    "\n",
    "print(\"--- ðŸš€ reconciling game scores s ðŸš€ ---\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# normalise scores for string comparison\n",
    "s1 = cleaned_matches_df['calcNestedGameScores'].astype(str).str.strip().replace({'nan': '', 'NaT': '', '<NA>': ''})\n",
    "s2 = cleaned_matches_df['B_rawGameScores'].astype(str).str.strip().replace({'nan': '', 'NaT': '', '<NA>': ''})\n",
    "s3 = cleaned_matches_df['A_rawGameScores'].astype(str).str.strip().replace({'nan': '', 'NaT': '', '<NA>': ''})\n",
    "\n",
    "# use a temporary score_df to do work on the series\n",
    "temp_scores_df = pd.DataFrame({'s1': s1, 's2': s2, 's3': s3}, index=cleaned_matches_df.index)\n",
    "\n",
    " \n",
    "def check_score_consistency(row):\n",
    "    \"\"\"\n",
    "    Checks if all available scores in the row are identical.\n",
    "    returns true if all scores agree, otherwise false\n",
    "    \"\"\"\n",
    "\n",
    "    # temp score df only contains scores so just need to compare all rows\n",
    "    valid_scores = set([s for s in row if s != \"\"])\n",
    "    \n",
    "    # if set is length 1 or 0 - scores are consistent. \n",
    "    return len(valid_scores) == 1\n",
    "\n",
    "# apply the scores_check\n",
    "scoreConsistenct = temp_scores_df.apply(check_score_consistency, axis=1)\n",
    "\n",
    "\n",
    "# Set dnf matches to be consistentScore = true, as these matches are a separate case.\n",
    "is_dnf = cleaned_matches_df['dnf'] != False \n",
    "\n",
    "# create column in the orginal to flag inconsistent scores (for future reference if needed)\n",
    "cleaned_matches_df['gameScoreConsistent'] = scoreConsistenct | is_dnf\n",
    "\n",
    "\n",
    "# compare all scores - if 2 or more agree - that value is used\n",
    "# otherwise scores are taken in order of the s1,s2,s3\n",
    "def get_democratic_game_score(row):\n",
    "    valid_scores = [s for s in row if s != \"\"]\n",
    "    if not valid_scores: return \"\"\n",
    "    mode_result = pd.Series(valid_scores).mode()\n",
    "    \n",
    "    if not mode_result.empty:\n",
    "        return mode_result.iloc[0] \n",
    "    else:\n",
    "        return row['s1'] # Fallback\n",
    "\n",
    "cleaned_matches_df['reconciledGameScore'] = temp_scores_df.apply(get_democratic_game_score, axis=1)\n",
    "# cleaned_matches_df.drop(columns=[\"resultOverallScores\",\"overallScores\", \"calcOverallScores\"],inplace=True, errors=\"ignore\")\n",
    "\n",
    "print(\"--- âœ”ï¸ Scores Triangulated / reconciliated âœ”ï¸ ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8667e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_game_score_validity(score_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Checks if a single game score (e.g., '11-8' or '13-11') is valid.\n",
    "    Returns a status string: 'Valid', 'Incomplete', or 'Invalid'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        home, away = map(int, score_str.split('-'))\n",
    "    except ValueError:\n",
    "        return \"Invalid_Format\"\n",
    "\n",
    "    if home == 0 and away == 0:\n",
    "        return \"Incomplete\"\n",
    "    \n",
    "    score_diff = abs(home - away)\n",
    "    winner_score = max(home, away)\n",
    "\n",
    "    if winner_score >= 11:\n",
    "        if score_diff >= 2:\n",
    "            return \"Valid\"\n",
    "        else:\n",
    "            return \"Invalid_Too_Close\" \n",
    "    else:\n",
    "        return \"Invalid_Too_Low\" \n",
    "def calculate_reconciled_game_scores_winner(row) -> pd.Series:\n",
    "# Use the cleaned game score string (e.g., '11-8,11-9,10-12')\n",
    "    score_string = row.get(\"reconciledGameScore\")\n",
    "\n",
    "    # Initialize the outputs\n",
    "    winner_result = \"error\"\n",
    "    game_status_flags = []\n",
    "\n",
    "    if not score_string:\n",
    "        winner_result = pd.NA\n",
    "    else:\n",
    "        games = score_string.split(\",\")\n",
    "        home_tally = 0\n",
    "        away_tally = 0\n",
    "        \n",
    "        for game_score_pair in games:\n",
    "            game_score_pair = game_score_pair.strip() # Clean up spaces\n",
    "            if not game_score_pair:\n",
    "                continue\n",
    "                \n",
    "            # Run the rule check for diagnostic column\n",
    "            game_status = check_game_score_validity(game_score_pair)\n",
    "            if game_status != \"Valid\":\n",
    "                game_status_flags.append(f\"{game_score_pair}:{game_status}\")\n",
    "\n",
    "            try:\n",
    "                home_score, away_score = map(int, game_score_pair.split('-'))\n",
    "            except ValueError:\n",
    "                # If a score can't be parsed, stop tallying\n",
    "                winner_result = \"Error_Parsing\"\n",
    "                break\n",
    "            \n",
    "            # Use Python's match/case for clean winner determination logic\n",
    "            match (home_score > away_score, away_score > home_score):\n",
    "                case (True, False):\n",
    "                    home_tally += 1\n",
    "                case (False, True):\n",
    "                    away_tally += 1\n",
    "                case _:\n",
    "                    continue\n",
    "\n",
    "        # Final match winner determination (only if no parsing error occurred)\n",
    "        if winner_result != \"Error_Parsing\":\n",
    "            if home_tally > away_tally:\n",
    "                winner_result = \"home\"\n",
    "            elif away_tally > home_tally:\n",
    "                winner_result = \"away\"\n",
    "            else:\n",
    "                winner_result = \"tie\"\n",
    "\n",
    "    # --- RETURN A SERIES ---\n",
    "    return pd.Series({\n",
    "        'calculatedGameScoreWinner': winner_result,\n",
    "        'gameScoreFlags': \"; \".join(game_status_flags)\n",
    "    })\n",
    "\n",
    "new_cols_df = cleaned_matches_df.apply(calculate_reconciled_game_scores_winner, axis=1)\n",
    "\n",
    "# Concat new data to original df \n",
    "cleaned_matches_df = pd.concat([cleaned_matches_df, new_cols_df], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f91bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark the flagged matches where a result is wrong \n",
    "flagged_matches = cleaned_matches_df[\"gameScoreFlags\"]!= \"\"\n",
    "not_dnf = cleaned_matches_df[\"dnf\"] == False\n",
    "winner_mismatch = (cleaned_matches_df[\"calculatedGameScoreWinner\"] != cleaned_matches_df[\"reconciledOverallScoreWinner\"])\n",
    "score_errors_df  = cleaned_matches_df[(flagged_matches |  winner_mismatch) & not_dnf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be59c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_errors_df.to_csv(\"../Data/Processed/Matches/score_errors.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bef5987",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_errors_fixed_df = pd.read_csv(\"../Data/Processed/Matches/score_errors_AMENDED.csv\", index_col=0)\n",
    "STABLE_KEY = ['eventId', 'documentCode']\n",
    "\n",
    "cleaned_matches_df = cleaned_matches_df.set_index(STABLE_KEY)\n",
    "score_errors_fixed_df = score_errors_fixed_df.set_index(STABLE_KEY)\n",
    "cleaned_matches_df.update(score_errors_fixed_df)\n",
    "cleaned_matches_df = cleaned_matches_df.reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11558cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_matches_df[winner_mismatch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remake = [\"gameScoreFlags\", \"calculatedGameScoreWinner\",\"reconciledOverallScoreWinner\"]\n",
    "cleaned_matches_df.drop(columns=cols_to_remake, inplace=True,errors=\"ignore\")\n",
    "cleaned_matches_df[\"reconciledOverallScoreWinner\"] = cleaned_matches_df.apply(get_overall_score_winner, axis=1)\n",
    "new_cols_df = cleaned_matches_df.apply(calculate_reconciled_game_scores_winner, axis=1)\n",
    "cleaned_matches_df = pd.concat([cleaned_matches_df, new_cols_df], axis=1)\n",
    "flagged_matches = cleaned_matches_df[\"gameScoreFlags\"]!= \"\"\n",
    "not_dnf = cleaned_matches_df[\"dnf\"] == False\n",
    "winner_mismatch = (cleaned_matches_df[\"calculatedGameScoreWinner\"] != cleaned_matches_df[\"reconciledOverallScoreWinner\"])\n",
    "score_errors_persistent_df = cleaned_matches_df[(flagged_matches | winner_mismatch) & not_dnf]\n",
    "\n",
    "if len(score_errors_persistent_df) == 0:\n",
    "    print(\"âœ… All Errors Resolved\")\n",
    "    print(f\"{len(score_errors_persistent_df)} Errors Remaining\")\n",
    "else:\n",
    "    print(f\"{len(score_errors_persistent_df)} Errors Remaining\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b5e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_of(row):\n",
    "    try:\n",
    "        scores = row[\"reconciledOverallScore\"].split(\"-\")\n",
    "        home_score = int(scores[0])\n",
    "        away_score = int(scores[1])\n",
    "        max_games_won = max(home_score, away_score)\n",
    "    \n",
    "        match max_games_won:\n",
    "            case 4:\n",
    "                return 7\n",
    "            case 3:\n",
    "                return 5\n",
    "            case _:\n",
    "                return 0\n",
    "    except:\n",
    "        return pd.NA\n",
    "    \n",
    "\n",
    "cleaned_matches_df[\"calcBestOf\"] = cleaned_matches_df.apply(get_best_of, axis=1)\n",
    "\n",
    "pd.to_numeric(cleaned_matches_df[\"calcBestOf\"],errors=\"coerce\")\n",
    "pd.to_numeric(cleaned_matches_df[\"bestOf\"],errors=\"coerce\")\n",
    "\n",
    "best_of_mismatch = cleaned_matches_df[\"calcBestOf\"] != cleaned_matches_df[\"bestOf\"]\n",
    "bestOf_missing = cleaned_matches_df[\"bestOf\"].isnull()\n",
    "\n",
    "\n",
    "\n",
    "# Only mismatches are for Paris Olympics 2024 (data error) and Macau World cup 24 and 25 (special cases)\n",
    "# The calcBestOf can be trusted it seems and taken as the source\n",
    "# however macau group games shall be set to best of 0 \n",
    "# macau world cup  2024 eventid = 2937, 2025 = 3109\n",
    "# Paris Olympics Event ID  = 2603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae3c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_matches_df[\"trueBestOf\"]= cleaned_matches_df[\"calcBestOf\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "world_cup_filter = cleaned_matches_df[\"eventId\"].isin([2937,3109])\n",
    "world_cup_group_filter = cleaned_matches_df[\"documentCode\"].str.contains(\"GP\")\n",
    "cleaned_matches_df.loc[world_cup_filter & world_cup_group_filter & not_dnf, \"trueBestOf\"] = 0\n",
    "\n",
    "paris_olympics_filter = cleaned_matches_df[\"eventId\"] == 2603\n",
    "paris_olympics_singles_filter = cleaned_matches_df[\"subEventName\"].str.contains(\"Singles\")\n",
    "cleaned_matches_df.loc[paris_olympics_filter & paris_olympics_singles_filter & not_dnf, \"trueBestOf\"] = int(7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f6c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_matches_df[\"trueBestOf\"]     = pd.to_numeric(cleaned_matches_df[\"trueBestOf\"],errors=\"coerce\")\n",
    "cleaned_matches_df[\"bestOf\"]     = pd.to_numeric(cleaned_matches_df[\"bestOf\"],errors=\"coerce\")\n",
    "best_of_mismatch = cleaned_matches_df[\"bestOf\"] != cleaned_matches_df[\"trueBestOf\"]\n",
    "bestOf_missing = cleaned_matches_df[not_dnf][\"trueBestOf\"].isnull()\n",
    "\n",
    "cleaned_matches_df[best_of_mismatch & ~bestOf_missing & ~world_cup_filter][[\"bestOf\",\"trueBestOf\",\"EventName\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5151ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "\n",
    "date_string = date.today().strftime(\"%Y%m%d\")\n",
    "file_name = f\"{date_string}_cleaned_matches.csv\"\n",
    "file_path = os.path.join(CLEANED_MATCHES_DIR, file_name)\n",
    "\n",
    "cleaned_matches_df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_filter =cleaned_matches_df[\"reconciledOverallScore\"].isnull() | cleaned_matches_df[\"reconciledGameScore\"].isnull()\n",
    "null_string_filter = (cleaned_matches_df[\"reconciledOverallScore\"].str.len()==0) | (cleaned_matches_df[\"reconciledGameScore\"].str.len()==0)\n",
    "cleaned_matches_df[null_filter | null_string_filter]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1965688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_mismatch = cleaned_matches_df[\"reconciledOverallScoreWinner\"]!= cleaned_matches_df[\"calculatedGameScoreWinner\"]\n",
    "cleaned_matches_df[winner_mismatch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d60fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "table_tennis_stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
