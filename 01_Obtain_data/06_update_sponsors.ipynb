{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e7aec68f-2be8-494a-b006-dd046f01e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import requests\n",
    "from typing import List, Dict, Union, Any, Optional\n",
    "from datetime import datetime\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import concurrent.futures\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "103e9b90-7b06-432e-bc39-ffae6f36877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- Configuration--------------------\n",
    "# Get current date for file naming.\n",
    "date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "\n",
    "INTERMEDIATE_DIR= \"../Data/Processed/Events/Intermediate/\"\n",
    "INTERMEDIATE_EVENTS_FILENAME_SUFFIX = \"_intermediate_events.csv\"\n",
    "INTERMEDIATE_EVENTS_REGEX = rf\"^\\d{{8}}{re.escape(INTERMEDIATE_EVENTS_FILENAME_SUFFIX)}$\"\n",
    "\n",
    "\n",
    "MASTER_EVENTS_DIR = \"../Data/Master/Events\"\n",
    "MASTER_EVENTS_FILENAME_SUFFIX = \"_master_events.csv\"\n",
    "MASTER_EVENTS_REGEX = rf\"^\\d{{8}}{re.escape(MASTER_EVENTS_FILENAME_SUFFIX)}$\"\n",
    "\n",
    "MASTER_OUTPUT_NAME = f\"{date}_events_master.csv\"\n",
    "MASTER_OUTPUT_PATH = os.path.join(MASTER_EVENTS_DIR, MASTER_OUTPUT_NAME)\n",
    "\n",
    "# Max number of threads for parallel calls.\n",
    "MAX_WORKERS = 20\n",
    "\n",
    "# Used to generate random pause times for API politeness.\n",
    "MIN_PAUSE = 0.1\n",
    "MAX_PAUSE = 0.2\n",
    "\n",
    "# Manual made table sponsor information \n",
    "EVENTS_TABLES_FILE = \"../Data/Processed/Sponsors/table_sponsors.csv\"\n",
    "EVENTS_TABLES_DF = pd.read_csv(EVENTS_TABLES_FILE)\n",
    "# Links to manually created json files mapping sponsor logos and links to brand names\n",
    "LOGOS_MAP_FILE = \"../Data/Processed/Sponsors/sponsor_logos_map.json\"\n",
    "LOGOS_MAP=json.load(open(LOGOS_MAP_FILE))\n",
    "\n",
    "LINKS_MAP_FILE = \"../Data/Processed/Sponsors/sponsor_links_map.json\"\n",
    "LINKS_MAP = json.load(open(LINKS_MAP_FILE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "31a97633-4e27-49dd-aec3-e0d6f68c72ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_master_events(master_dir:str, master_regex) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Parses specified directory for events files in format yyyy_mm_dd. \n",
    "    Attempts to read latest file in this format. \n",
    "\n",
    "    Args:\n",
    "        directory (str): The folder where the master files are stored (e.g., '../Data/Master/Events').\n",
    "        filename_pattern (str): The pattern to match (e.g., '*_events_master.csv').\n",
    "\n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: The DataFrame of the latest file, or None if no files are found or reading fails.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(master_dir):\n",
    "        print (f\"❌{master_dir} does not exist as a directory\")\n",
    "        return None \n",
    "    \n",
    "    # Get csv files in \n",
    "    files = glob.glob(f\"{master_dir}/*.csv\")\n",
    "   \n",
    "\n",
    "    master_files = []\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"❌ No existing *.csv files found in MASTER Events Directory: {master_dir} \")\n",
    "        return None\n",
    "\n",
    "    for file in files:\n",
    "        filename = os.path.basename(file)\n",
    "       \n",
    "        if re.match(master_regex,filename):\n",
    "            master_files.append(file)\n",
    "\n",
    "    if not master_files:\n",
    "        print(f\"❌ No existing MASTER files in format: {master_regex} in {master_dir} \")\n",
    "        return None\n",
    "    master_files.sort()    \n",
    "    latest_master = master_files[-1]\n",
    "\n",
    "    try: \n",
    "        latest_master_df = pd.read_csv(latest_master)\n",
    "        print(f\"✅ {len(latest_master_df)} events found in latest MASTER: {file} \")\n",
    "        return latest_master_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print (f\"❌ Error reading lastest MASTER, {file}: {e}\")\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e2b2f78f-7de8-49c2-a2bb-237b58a37670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_intermediate_events(intermediate_dir:str, intermediate_regex) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Parses specified directory for events files in format yyyy_mm_dd. \n",
    "    Attempts to read latest file in this format. \n",
    "\n",
    "    Args:\n",
    "        directory (str): The folder where the intermediate files are stored (e.g., '../Data/Events/Intermediate').\n",
    "        filename_pattern (str): The pattern to match (e.g., '*_events_intermediate.csv').\n",
    "\n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: The DataFrame of the latest file, or None if no files are found or reading fails.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(intermediate_dir):\n",
    "        print (f\"❌{intermediate_dir} does not exist as a directory\")\n",
    "        return None \n",
    "    \n",
    "    # Get csv files in \n",
    "    files = glob.glob(f\"{intermediate_dir}/*.csv\")\n",
    "   \n",
    "\n",
    "    intermediate_files = []\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"❌ No existing *.csv files found in INTERMEDIATE Events Directory: {intermediate_dir} \")\n",
    "        return None\n",
    "\n",
    "    for file in files:\n",
    "        filename = os.path.basename(file)\n",
    "       \n",
    "        if re.match(intermediate_regex,filename):\n",
    "           intermediate_files.append(file)\n",
    "\n",
    "    if not intermediate_files:\n",
    "        print(f\"❌ No existing INTERMEDIATE files in format: {intermediate_regex} in {intermediate_dir}\")\n",
    "        return None\n",
    "    intermediate_files.sort()    \n",
    "    latest_intermediate = intermediate_files[-1]\n",
    "\n",
    "    try: \n",
    "        latest_intermediate_df = pd.read_csv(latest_intermediate)\n",
    "        print(f\"✅ {len(latest_intermediate_df)} events found in latest INTERMEDIATE: {file} \")\n",
    "        return latest_intermediate_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print (f\"❌ Error reading lastest INTERMEDIATE, {file}: {e}\")\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7c50eb17-9a28-477f-a299-eaf49af3efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unenriched_event_ids(df: pd.DataFrame) -> List[int]:\n",
    "    \"\"\"\n",
    "    Identifies events in the DataFrame that already have valid sponsor data \n",
    "    (i.e., BallSponsor or TableSponsor is not 'TBC' and not None).\n",
    "    Returns a list of these event is.\n",
    "    \"\"\"\n",
    "    # Identify rows where either sponsor column is NOT 'TBC' AND NOT NaN.\n",
    "    #'|' to see if either column has data \n",
    "    # TBC means no sponsor data has been searched for \n",
    "    # None means no sponsor data was found previously\n",
    "    unenriched_mask = (\n",
    "        (df['BallSponsor'] == 'TBC') & (df['BallSponsor'].notna())\n",
    "    ) | (\n",
    "        (df['TableSponsor'] == 'TBC') & (df['TableSponsor'].notna())\n",
    "    )\n",
    "    \n",
    "    \n",
    "    ids = list(set(df[unenriched_mask]['eventId'].tolist()))\n",
    "\n",
    "   \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "caa44250-e151-4ca1-9ae5-59f06977ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sponsors(event_id:Union[int,str], min_pause:float, max_pause:float) -> Optional[List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Function used for parralel, threaded api calls to fetch sponsor details,\n",
    "    for one event specified by event_id. Returns JSON Dict of the \n",
    "\n",
    "    Args:\n",
    "        event_id (int): The unique id of the event.\n",
    "        min_pause (float): Minimum pause duration (seconds).\n",
    "        max_pause (float): Maximum pause duration (seconds).\n",
    "\n",
    "    Returns:\n",
    "        (sponsors_list)  A list of sponsors data. \n",
    "    \"\"\"\n",
    "\n",
    "    # initialise variables to be returned in the final dictionary.\n",
    "   \n",
    "    \n",
    "    sponsors_list = None    \n",
    "\n",
    "    \n",
    "    # define api url and headers.\n",
    "    url = f\"https://wtt-website-api-prod-3-frontdoor-bddnb2haduafdze9.a01.azurefd.net/api/cms/GetEventEquipmentwithLogo/{event_id}\" \n",
    "    headers = {\n",
    "        \"accept\": \"application/json, text/plain, */*\",\n",
    "        \"accept-language\": \"en-GB,en;q=0.9,es=q=0.8\",\n",
    "        \"cache-control\": \"no-cache\",\n",
    "        \"dnt\": \"1\",\n",
    "        \"origin\": \"https://www.worldtabletennis.com\",\n",
    "        \"pragma\": \"no-cache\",\n",
    "        \"priority\": \"u=1, i\",\n",
    "        \"referer\": \"https://www.worldtabletennis.com/\",\n",
    "        \"sec-ch-ua\": \"\\\"Chromium\\\";v=\\\"140\\\", \\\"Not=A?Brand\\\";v=\\\"24\\\", \\\"Google Chrome\\\";v=\\\"140\\\"\",\n",
    "        \"sec-ch-ua-mobile\": \"?1\",\n",
    "        \"sec-ch-ua-platform\": \"\\\"Android\\\"\",\n",
    "        \"sec-fetch-dest\": \"empty\",\n",
    "        \"sec-fetch-mode\": \"cors\",\n",
    "        \"sec-fetch-site\": \"cross-site\",\n",
    "        \"secapimkey\": \"S_WTT_882jjh7basdj91834783mds8j2jsd81\",\n",
    "        \"user-agent\": \"Mozilla/5.0 (Linux; Android 11.0; Surface Duo) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Mobile Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # make the api call and get response as a json. Raise errors if they occur\n",
    "    try:\n",
    "        # longer timeout \n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status() # Raise an error for bad status codes (4xx or 5xx)\n",
    "        sponsors_json = response.json()\n",
    "\n",
    "        #  check that response contains data and is a list:        \n",
    "        if sponsors_json and isinstance(sponsors_json,list):\n",
    "            sponsors_list = sponsors_json\n",
    "            sleep_duration = random.uniform(min_pause, max_pause)\n",
    "            time.sleep(sleep_duration)\n",
    "    \n",
    "            # Return the list (can be empty if API returned [], or None if checks failed)\n",
    "            return sponsors_list, event_id \n",
    "   \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        status_msg = f\"HTTP Error {e.response.status_code}\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        status_msg = \"Timeout\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "         status_msg = f\"Request Error {type(e).__name__}\"\n",
    "    except Exception as e: # Catch any other unexpected errors\n",
    "        status_msg = f\"Unexpected Error {type(e).__name__}\"\n",
    "\n",
    "    # Pause for API politness before returning None and continuing\n",
    "    sleep_duration = random.uniform(min_pause, max_pause)\n",
    "    time.sleep(sleep_duration)\n",
    "    return None, event_id\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1e91e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sponsors_and_overwrite(intermediate_df, balls_df, tables_df):\n",
    "    \"\"\"\n",
    "    Merges sponsor data onto the intermediate DataFrame, overwriting placeholders \n",
    "    and ensuring all missing values are represented by NaN (or None).\n",
    "    \"\"\"\n",
    "    \n",
    "   # peplace 'TBC' with NaN as this data is now known to be unavailable (save for manual additions)\n",
    "\n",
    "    intermediate_df.replace('TBC', pd.NA, inplace=True)    \n",
    "    merged_df = intermediate_df.copy()\n",
    "\n",
    "    # --- 2. Merge Ball Sponsors ---\n",
    "    \n",
    "    # Merge the new BallSponsor data. The original column is renamed with suffix '_x'.\n",
    "    merged_df = merged_df.merge(\n",
    "        balls_df[['eventId', 'BallSponsor']],\n",
    "        on='eventId',\n",
    "        how='left',\n",
    "        suffixes=('_original', '_new') \n",
    "    )\n",
    "\n",
    "   \n",
    "    # We use .combine_first() to achieve this:\n",
    "    # If '_new' has a value (not NaN/None), use it. \n",
    "    # If '_new' is NaN/None, use the value from '_original' (which is also NaN/None if TBC was replaced).\n",
    "    merged_df['BallSponsor'] = merged_df['BallSponsor_new'].combine_first(merged_df['BallSponsor_original'])\n",
    "    \n",
    "    # Drop the temporary merged columns\n",
    "    merged_df.drop(columns=['BallSponsor_original', 'BallSponsor_new'], inplace=True)\n",
    "    \n",
    "    \n",
    "    # --- 3. Merge Table Sponsors ---\n",
    "    \n",
    "    # Merge the new TableSponsor data.\n",
    "    merged_df = merged_df.merge(\n",
    "        tables_df[['eventId', 'TableSponsor']],\n",
    "        on='eventId',\n",
    "        how='left',\n",
    "        suffixes=('_original', '_new')\n",
    "    )\n",
    "\n",
    "    # Overwrite the original column\n",
    "    merged_df['TableSponsor'] = merged_df['TableSponsor_new'].combine_first(merged_df['TableSponsor_original'])\n",
    "    \n",
    "    # Drop the temporary merged columns\n",
    "    merged_df.drop(columns=['TableSponsor_original', 'TableSponsor_new'], inplace=True)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "110154c9-d685-494c-9e8c-6c43078c2875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No existing MASTER files in format: ^\\d{8}_master_events\\.csv$ in ../Data/Master/Events \n",
      "✅ 184 events found in latest INTERMEDIATE: ../Data/Processed/Events/Intermediate/20251027_intermediate_events.csv \n",
      "Using intermediate file (184 events) for enrichment.\n",
      "\n",
      "---🚀 Starting Concurrent Scraping for 184 Events 🚀---\n",
      "--- Using 20 threads. API pause: 0.1s - 0.2s ---\n",
      "✅ Finished! Sponsor data collected for 161/184 events.\n",
      "Time Elapsed = 3.7882885932922363\n",
      "---🟢 Scraping finished. 🟢---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # record time for logging purposes.\n",
    "    start_time = time.time()\n",
    "\n",
    "     # Ensure the master output directory exists for the check\n",
    "    os.makedirs(MASTER_EVENTS_DIR, exist_ok=True) \n",
    "    # Ensure the intermediate output directory exists for saving the output\n",
    "    os.makedirs(INTERMEDIATE_DIR, exist_ok=True)\n",
    "    \n",
    "    # Check if a master file already exists: returns None if no master is found\n",
    "    master_df = get_latest_master_events(MASTER_EVENTS_DIR, MASTER_EVENTS_REGEX)\n",
    "\n",
    "    if master_df is not None and not master_df.empty:\n",
    "        base_df = master_df.copy()\n",
    "        # Placeholder - need to consider how to handle this ? \n",
    "        print(f\"sing existing master file as base data ({len(base_df)} events).\")\n",
    "    else:\n",
    "        # check for intermediate files\n",
    "        intermediate_df = get_latest_intermediate_events(INTERMEDIATE_DIR, INTERMEDIATE_EVENTS_REGEX)\n",
    "        \n",
    "        if intermediate_df is not None and not intermediate_df.empty:\n",
    "            base_df = intermediate_df.copy()\n",
    "            print(f\"Using intermediate file ({len(base_df)} events) for enrichment.\")\n",
    "        else:\n",
    "            print(\"--- ❌ FATAL ERROR: No base file (Master or Intermediate) found. Cannot proceed. ---\")\n",
    "            sys.exit(1)\n",
    "\n",
    "   \n",
    "    ids_to_get_sponsors = get_unenriched_event_ids(intermediate_df) \n",
    "    ids_to_get_sponsors_count = len(ids_to_get_sponsors)\n",
    "\n",
    "    # if nothing to get sponsors for - sys.exit(0) as the job is done (or something went wrong?)\n",
    "    if ids_to_get_sponsors_count == 0:\n",
    "        print(\"\\n--- ✅ All events are already enriched. ---\")\n",
    "        time_taken = time.time() - start_time\n",
    "        print(f\"Total time taken = {time_taken:.2f} s.\")\n",
    "        print(\"---🟢 Scraping finished. 🟢---\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    \n",
    "    print(f\"\\n---🚀 Starting Concurrent Scraping for {ids_to_get_sponsors_count} Events 🚀---\")\n",
    "    print(f\"--- Using {MAX_WORKERS} threads. API pause: {MIN_PAUSE:.1f}s - {MAX_PAUSE:.1f}s ---\")\n",
    "    \n",
    "    # initialise list and counts to be added to.\n",
    "    all_raw_sponsors_list = []\n",
    "    processed_count = 0\n",
    "    success_count = 0\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "\n",
    "        futures = {\n",
    "            executor.submit(get_sponsors, event_id, MIN_PAUSE, MAX_PAUSE): event_id\n",
    "            for event_id in ids_to_get_sponsors}\n",
    "\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            processed_count +=1\n",
    "            try:\n",
    "                result, event_id = future.result()\n",
    "\n",
    "                if result is not None:\n",
    "                    all_raw_sponsors_list.extend(result)\n",
    "                    success_count += 1 \n",
    "            except Exception as e:\n",
    "                print(f\"Error with event id {event_id}\")\n",
    "                \n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "   \n",
    "\n",
    "    print(f\"✅ Finished! Sponsor data collected for {success_count}/{ids_to_get_sponsors_count} events.\")\n",
    "    print(f\"Time Elapsed = {time_taken}\")\n",
    "    print(\"---🟢 Scraping finished. 🟢---\")\n",
    "    \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5e2d5265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enriched master events file saved to: ../Data/Master/Events/20251029_events_master.csv \n"
     ]
    }
   ],
   "source": [
    "# Drop the intermeddiate placeholder sponsor columns if they exist\n",
    "try:\n",
    "    intermediate_df.drop(columns=['BallSponsor', 'TableSponsor'], inplace=True)\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "sponsors_df = pd.DataFrame(all_raw_sponsors_list)\n",
    "\n",
    "# filter for equipment sponsors (balls and tables)\n",
    "equipment_mask = (\n",
    "    (sponsors_df['sponsorTypeName'].str.contains(\"ball\", case=False)) | \n",
    "    (sponsors_df['sponsorTypeName'].str.contains(\"table\", case=False))\n",
    ")\n",
    "# Create EQUIPMENT sponsors dataframe\n",
    "equipment_df = sponsors_df[equipment_mask].copy()\n",
    "all_equipment_df = equipment_df.copy()\n",
    "equipment_df[\"Brand\"] = None\n",
    "\n",
    "# Where link is available, use that to map to brand name using LINKS_MAP (manually created)\n",
    "equipment_df['Brand'] = equipment_df['sponsorLink'].map(lambda x: LINKS_MAP.get(x, None))\n",
    "\n",
    "#Fill in missing Brand names using logo mapping - \n",
    "#Only apply logo mapping to rows where Brand is still missing\n",
    "no_links_mask = equipment_df[\"Brand\"].isna()\n",
    "\n",
    "\n",
    "mapped_logo_values = equipment_df.loc[no_links_mask, \"logo\"].map(\n",
    "    lambda x: LOGOS_MAP.get(x, None) \n",
    ")\n",
    "equipment_df.loc[no_links_mask, \"Brand\"] = mapped_logo_values\n",
    "\n",
    "# get balls and tables dataframes separately\n",
    "balls_df = equipment_df[equipment_df['sponsorTypeName'].str.contains(\"ball\", case=False)].copy()\n",
    "balls_df.rename(columns={\"Brand\": \"BallSponsor\"}, inplace=True)\n",
    "balls_df = balls_df[['eventId', 'BallSponsor']].copy()\n",
    "\n",
    "tables_df = equipment_df[equipment_df['sponsorTypeName'].str.contains(\"table\", case=False)].copy()\n",
    "tables_df.rename(columns={\"Brand\": \"TableSponsor\"}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Add the manual table sponsor data from EVENTS_TABLES_DF\n",
    "enriched_df = pd.merge(left=intermediate_df, right=EVENTS_TABLES_DF, on='eventId', how='left')\n",
    "\n",
    "# get events where no manual data has been made yet!\n",
    "\n",
    "no_table_sponsor_mask = enriched_df['TableSponsor'].isna()\n",
    "no_table_df = enriched_df[no_table_sponsor_mask]\n",
    "no_table_ids = no_table_df['eventId'].tolist()\n",
    "\n",
    "for event_id in no_table_ids:\n",
    "    table_sponsor = tables_df.loc[tables_df['eventId'] == event_id, 'TableSponsor'].values[0]\n",
    "    enriched_df.loc[enriched_df['eventId'] == event_id, 'TableSponsor'] = table_sponsor\n",
    "\n",
    "enriched_df = pd.merge(left=enriched_df, right=balls_df, on='eventId', how='left')\n",
    "\n",
    "enriched_df.to_csv(MASTER_OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"✅ Enriched master events file saved to: {MASTER_OUTPUT_PATH} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d66ac957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TableSponsor\n",
       "DHS            44\n",
       "Stag           31\n",
       "Butterfly      30\n",
       "Tibhar         24\n",
       "Double Fish    15\n",
       "Joola          12\n",
       "Donic          10\n",
       "Stiga           9\n",
       "Yinhe           4\n",
       "San-Ei          2\n",
       "Andro           2\n",
       "Nittaku         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched_df[\"TableSponsor\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97daad1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "table_tennis_stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
